{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pgBTRErO7ZTI",
        "ahcSk89NtLJr",
        "mOvEvlGvXktn",
        "T5v7S_aXYFAk",
        "KrC0GQTzfXJP"
      ],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c64beb169fed4ae4bb9d637d008e3dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd089bfa14aa485696598bdf7fa79e7b",
              "IPY_MODEL_6ec0c1575d344a358d684d6a0188ebfa",
              "IPY_MODEL_68570864f650495ea866a421f571db5e"
            ],
            "layout": "IPY_MODEL_a05ae6830dba4317ba4fee48c4b8ba88"
          }
        },
        "fd089bfa14aa485696598bdf7fa79e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34099e3960424c6e92599271dda23882",
            "placeholder": "​",
            "style": "IPY_MODEL_7c34e9e4643445b4af62df1fc8952a16",
            "value": "Parsing nodes: 100%"
          }
        },
        "6ec0c1575d344a358d684d6a0188ebfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_899beadf482343ef80df2dc950783ce4",
            "max": 8065,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a521b10369a64d299c437f9b86ef2da4",
            "value": 8065
          }
        },
        "68570864f650495ea866a421f571db5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f451686732154ec5ad625a47b2995ea5",
            "placeholder": "​",
            "style": "IPY_MODEL_5e82cb45e0ce4372b04412c089732bdd",
            "value": " 8065/8065 [00:01&lt;00:00, 2414.02it/s]"
          }
        },
        "a05ae6830dba4317ba4fee48c4b8ba88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34099e3960424c6e92599271dda23882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c34e9e4643445b4af62df1fc8952a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "899beadf482343ef80df2dc950783ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a521b10369a64d299c437f9b86ef2da4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f451686732154ec5ad625a47b2995ea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e82cb45e0ce4372b04412c089732bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f35f8b74579b4d60bba2bdee3f74dbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0a63503152746a9b5d9663bdf3348e3",
              "IPY_MODEL_ba1f7b49f3a6481daae3ea01982f73d8",
              "IPY_MODEL_04b3dc2d6b3d44a1a3519e07d87091b8"
            ],
            "layout": "IPY_MODEL_4c1deab500a140b18ea930e7508eee56"
          }
        },
        "c0a63503152746a9b5d9663bdf3348e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a4cdabbbcc34147b1c610c7326f2286",
            "placeholder": "​",
            "style": "IPY_MODEL_246c2e097ca64eb3b86bb82a8694a4d0",
            "value": "Generating embeddings: 100%"
          }
        },
        "ba1f7b49f3a6481daae3ea01982f73d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6031c9f0dfe04cd4bcf05c7b8616b77c",
            "max": 2048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2d402e163334b8bad24f2469d92ad87",
            "value": 2048
          }
        },
        "04b3dc2d6b3d44a1a3519e07d87091b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a773c22dbf4f4eb5761593eef6aa3d",
            "placeholder": "​",
            "style": "IPY_MODEL_584ba22d68324043b33d470f3d3983e2",
            "value": " 2048/2048 [00:56&lt;00:00, 38.36it/s]"
          }
        },
        "4c1deab500a140b18ea930e7508eee56": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a4cdabbbcc34147b1c610c7326f2286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "246c2e097ca64eb3b86bb82a8694a4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6031c9f0dfe04cd4bcf05c7b8616b77c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2d402e163334b8bad24f2469d92ad87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09a773c22dbf4f4eb5761593eef6aa3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "584ba22d68324043b33d470f3d3983e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c502951e8e34ef2bde13e51c757ce0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_034da5b0d12b4736af17b48d72174160",
              "IPY_MODEL_78bbd07147414f738421107303d474f5",
              "IPY_MODEL_304278c9f647471382f682bc72acc3e3"
            ],
            "layout": "IPY_MODEL_0d30f4b3b8de4683a6792669220e1785"
          }
        },
        "034da5b0d12b4736af17b48d72174160": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec1d6d89cee3418fa4ba790674934510",
            "placeholder": "​",
            "style": "IPY_MODEL_8e04688656384706be2718d171c729ee",
            "value": "Generating embeddings: 100%"
          }
        },
        "78bbd07147414f738421107303d474f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f68f2ff76d63432d80151a556fb334a8",
            "max": 2048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e77d1c58792402ab4b3bcac00d3b768",
            "value": 2048
          }
        },
        "304278c9f647471382f682bc72acc3e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47ecebec132e4819a1e55b860d52cc5b",
            "placeholder": "​",
            "style": "IPY_MODEL_228ff347a73b4ae984149278bfbbae47",
            "value": " 2048/2048 [01:18&lt;00:00, 19.28it/s]"
          }
        },
        "0d30f4b3b8de4683a6792669220e1785": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec1d6d89cee3418fa4ba790674934510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e04688656384706be2718d171c729ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f68f2ff76d63432d80151a556fb334a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e77d1c58792402ab4b3bcac00d3b768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47ecebec132e4819a1e55b860d52cc5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "228ff347a73b4ae984149278bfbbae47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "502332c6273f4563afb6a2a9ac31a706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_523e9a3ba2f942158e80edca4cb208b2",
              "IPY_MODEL_59f4c48c8ceb4d9f8039f5662fdd5d49",
              "IPY_MODEL_410822ff67d54e90889df67dd03286e6"
            ],
            "layout": "IPY_MODEL_00397620d3284f2188e861c53b67850a"
          }
        },
        "523e9a3ba2f942158e80edca4cb208b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fcbb90025594ab7a966883210252952",
            "placeholder": "​",
            "style": "IPY_MODEL_b49c3383e85c427b8e94d44a5467d9db",
            "value": "Generating embeddings: 100%"
          }
        },
        "59f4c48c8ceb4d9f8039f5662fdd5d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec3d72ecf9e94834bb29ccc2e3a6171f",
            "max": 2048,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f62612ba302f42cabe84638c2f1c38c7",
            "value": 2048
          }
        },
        "410822ff67d54e90889df67dd03286e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_326239c73f9b441fa58853039bdfe420",
            "placeholder": "​",
            "style": "IPY_MODEL_2d7e3315f40c4ecc94585db12ed8388e",
            "value": " 2048/2048 [01:33&lt;00:00, 21.38it/s]"
          }
        },
        "00397620d3284f2188e861c53b67850a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fcbb90025594ab7a966883210252952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b49c3383e85c427b8e94d44a5467d9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec3d72ecf9e94834bb29ccc2e3a6171f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f62612ba302f42cabe84638c2f1c38c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "326239c73f9b441fa58853039bdfe420": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d7e3315f40c4ecc94585db12ed8388e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6aa6b9bbdb814560948240703f8c6ef3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_54665708da524058bc874af25852f3c7",
              "IPY_MODEL_c2233649cdfe4d73bd8e743d66c82061",
              "IPY_MODEL_694762c150dc43a98492676bebbc9832"
            ],
            "layout": "IPY_MODEL_2f2bb5f6e5714c4b95f4119ee2a10085"
          }
        },
        "54665708da524058bc874af25852f3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea559a9a06584526a0749b56b23c19a6",
            "placeholder": "​",
            "style": "IPY_MODEL_2debcdbcb5ab42d7b5f9240a15408691",
            "value": "Generating embeddings: 100%"
          }
        },
        "c2233649cdfe4d73bd8e743d66c82061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae01dcb5e6b94965a2eadaf36ed5f1a7",
            "max": 2023,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa7f7d6568f34b6cb28fa1bf61ec4764",
            "value": 2023
          }
        },
        "694762c150dc43a98492676bebbc9832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3aab0613ff3400c94439fd51dc95465",
            "placeholder": "​",
            "style": "IPY_MODEL_eca6efcf157b49efbbb48e737adba493",
            "value": " 2023/2023 [01:29&lt;00:00, 19.57it/s]"
          }
        },
        "2f2bb5f6e5714c4b95f4119ee2a10085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea559a9a06584526a0749b56b23c19a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2debcdbcb5ab42d7b5f9240a15408691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae01dcb5e6b94965a2eadaf36ed5f1a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7f7d6568f34b6cb28fa1bf61ec4764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3aab0613ff3400c94439fd51dc95465": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eca6efcf157b49efbbb48e737adba493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TRABAJO PRACTICO FINAL - PROCESAMIENTO DEL LENGUAJE NATURAL\n",
        "\n",
        "**Tecnicatura Universitaria en Inteligencia Artificial**  \n",
        "**Universidad Nacional de Rosario**\n",
        "\n",
        "---\n",
        "\n",
        "**Materia:** IA 4.2 - Procesamiento del Lenguaje Natural  \n",
        "**Estudiante:** Sebastián Palacio  \n",
        "**DNI:** 43491996\n",
        "**Fecha de entrega:** 7 de diciembre de 2025\n",
        "\n",
        "**Docentes:**\n",
        "- Juan Pablo Manson\n",
        "- Alan Geary\n",
        "- Constantino Ferrucci\n",
        "\n",
        "---\n",
        "\n",
        "## Descripcion del Trabajo\n",
        "\n",
        "Este notebook implementa un sistema completo de asistente virtual para una empresa de electrodomesticos utilizando:\n",
        "- **Ejercicio 1:** Sistema RAG con 3 bases de datos (vectorial, tabular, grafos)\n",
        "- **Ejercicio 2:** Agente autonomo con patron ReAct y 4 herramientas especializadas\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6U8A7m-qaw2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Parte 1**"
      ],
      "metadata": {
        "id": "0mOuWlRH694b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/NLP/todo.zip\" -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byq9vK0QqB_R",
        "outputId": "c36352e2-2fd6-452c-ce97-ba88153d5ca0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/NLP/todo.zip\n",
            "replace /content/productos.xlsx? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Distribución por tipo de base de datos**"
      ],
      "metadata": {
        "id": "pgBTRErO7ZTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. BASE DE DATOS VECTORIAL (ChromaDB/FAISS)\n",
        "\n",
        "Contenido:\n",
        "  - FAQs (3,000 preguntas-respuestas)\n",
        "  - Manuales de productos (50 manuales)\n",
        "  - [OPCIONAL] Reseñas de usuarios (5,015 reseñas)\n",
        "\n",
        "Justificación:\n",
        "  - Información textual no estructurada\n",
        "  - Ideal para búsqueda semántica\n",
        "  - Responde preguntas como:\n",
        "    * \"¿Cómo uso mi licuadora para hacer smoothies?\"\n",
        "    * \"¿Qué voltaje requiere el rallador digital eléctrico?\"\n",
        "    * \"¿Qué opinan los usuarios de esta cafetera?\"\n",
        "\n",
        "\n",
        "2. BASE DE DATOS TABULAR (Pandas/SQL)\n",
        "\n",
        "Contenido:\n",
        "  - productos.csv (precio, stock, specs técnicas)\n",
        "  - inventario_sucursales.csv (stock por sucursal)\n",
        "  - ventas_historicas.csv (para analytics_tool en Ejercicio 2)\n",
        "\n",
        "Justificación:\n",
        "  - Datos estructurados con campos numéricos y categóricos\n",
        "  - Requiere filtros y comparaciones exactas\n",
        "  - Responde preguntas como:\n",
        "    * \"¿Cuáles son las licuadoras de menos de $200?\"\n",
        "    * \"¿Qué productos están en la categoría Cocina?\"\n",
        "    * \"Quiero una licuadora con buenas reseñas\"\n",
        "\n",
        "\n",
        "3. BASE DE DATOS DE GRAFOS (Neo4j/NetworkX)\n",
        "\n",
        "Contenido (Relaciones):\n",
        "  - Producto -> Categoría -> Subcategoría\n",
        "  - Producto -> Vendedor (desde ventas_historicas)\n",
        "  - Vendedor -> Sucursal\n",
        "  - Producto -> Tickets de soporte\n",
        "  - Cliente -> Provincia -> Compras\n",
        "\n",
        "Justificación:\n",
        "  - Datos interconectados con múltiples relaciones\n",
        "  - Consultas de tipo \"caminos\" y \"conexiones\"\n",
        "  - Responde preguntas como:\n",
        "    * \"¿Qué productos están relacionados con Cocina?\"\n",
        "    * \"¿Qué vendedores venden productos de la marca X?\"\n",
        "    * \"¿Qué productos tienen más tickets de soporte?\""
      ],
      "metadata": {
        "id": "8DON8RJ-vD0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Justificación de Framework"
      ],
      "metadata": {
        "id": "3b75x8t3U0Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "framework_analysis = \"\"\"\n",
        "DECISIÓN: Uso de LlamaIndex en lugar de Langchain\n",
        "\n",
        "CONTEXTO DEL ENUNCIADO:\n",
        "El TP especifica: \"Utilizar Langchain para implementar un agente basado\n",
        "en el paradigma ReAct\"\n",
        "\n",
        "DECISIÓN TOMADA:\n",
        "Se utilizó LlamaIndex para todo el proyecto (Ejercicio 1 y 2)\n",
        "\n",
        "JUSTIFICACIÓN TÉCNICA:\n",
        "\n",
        "COMPATIBILIDAD CON MATERIAL DE CLASE:\n",
        "   - El material TUIA_NLP_Unidad_7.ipynb usa LlamaIndex y me guié principalmente por los contenidos de clase\n",
        "   - Los ejemplos de ReAct en clase están en LlamaIndex\n",
        "   - Metodología enseñada: LlamaIndex para RAG + Agentes\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qDlrJsB4UzcV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preparación de dependencias y librerías**"
      ],
      "metadata": {
        "id": "3AUpirjuvX2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install llama_index==0.12.39 llama-index-embeddings-huggingface==0.5.4 sentence-transformers==4.1.0\n",
        "!pip install rank_bm25 unidecode nltk\n",
        "!pip install neo4j networkx matplotlib\n",
        "!pip install faiss-cpu pandas\n",
        "!pip install -q chromadb langchain langchain-community sentence-transformers rank-bm25\n",
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "Nwss6K7QEmsg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any\n",
        "import time\n",
        "\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import VectorStoreIndex, Document as LlamaDocument, Settings\n",
        "from llama_index.core.schema import TextNode, NodeWithScore\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
        "\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sqlite3\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "ruta_base = '/content/'\n"
      ],
      "metadata": {
        "id": "4msMt07Oo4eY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "353a41d3-761c-4a97-9000-97e42f28c71b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuración del modelo de embeddings"
      ],
      "metadata": {
        "id": "5EQQQglAs0jo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Cargando modelo de embeddings para español...')\n",
        "\n",
        "model_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "#Configurar globalmente en LlamaIndex\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=model_name)\n",
        "\n",
        "print(f\"Modelo configurado: {model_name}\")\n",
        "\n",
        "#Prueba\n",
        "test_embedding = Settings.embed_model.get_text_embedding(\"¿Cómo uso mi licuadora?\")\n",
        "print(f\"Dimensión del embedding: {len(test_embedding)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTDDYHOZMzWn",
        "outputId": "a2a9b63d-3ec8-40ee-b1ee-90d520807c65"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelo de embeddings para español...\n",
            "Modelo configurado: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
            "Dimensión del embedding: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carga de datos"
      ],
      "metadata": {
        "id": "ahcSk89NtLJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FAQs\n",
        "print(\"\\n1. Cargando FAQs...\")\n",
        "with open(ruta_base + 'faqs.json', 'r', encoding='utf-8') as f:\n",
        "    faqs = json.load(f)\n",
        "print(f\"   ✓ {len(faqs)} FAQs cargados\")\n",
        "\n",
        "#Manuales\n",
        "print(\"\\n2. Cargando manuales...\")\n",
        "manuales_dir = ruta_base + 'manuales_productos/'\n",
        "manuales = []\n",
        "\n",
        "for archivo in os.listdir(manuales_dir):\n",
        "    if archivo.endswith('.md'):\n",
        "        with open(os.path.join(manuales_dir, archivo), 'r', encoding='utf-8') as f:\n",
        "            contenido = f.read()\n",
        "\n",
        "        id_producto = archivo.split('_')[1]\n",
        "        manuales.append({\n",
        "            'id_producto': id_producto,\n",
        "            'nombre_archivo': archivo,\n",
        "            'contenido': contenido\n",
        "        })\n",
        "\n",
        "print(f\"   ✓ {len(manuales)} manuales cargados\")\n",
        "\n",
        "#CSVs para base tabular\n",
        "print(\"\\n3. Cargando datos tabulares...\")\n",
        "df_productos = pd.read_csv(ruta_base + 'productos.csv')\n",
        "df_ventas = pd.read_csv(ruta_base + 'ventas_historicas.csv')\n",
        "df_inventario = pd.read_csv(ruta_base + 'inventario_sucursales.csv')\n",
        "\n",
        "print(f\"   ✓ Productos: {len(df_productos)} registros\")\n",
        "print(f\"   ✓ Ventas: {len(df_ventas)} registros\")\n",
        "print(f\"   ✓ Inventario: {len(df_inventario)} registros\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFHSxbfmM2st",
        "outputId": "cf90abba-96e0-46b7-a04c-e3dac3d5cc53"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Cargando FAQs...\n",
            "   ✓ 3000 FAQs cargados\n",
            "\n",
            "2. Cargando manuales...\n",
            "   ✓ 50 manuales cargados\n",
            "\n",
            "3. Cargando datos tabulares...\n",
            "   ✓ Productos: 300 registros\n",
            "   ✓ Ventas: 10000 registros\n",
            "   ✓ Inventario: 4100 registros\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "#Cargar documentos\n",
        "documents = []\n",
        "with open('/content/faqs.json', 'r', encoding='utf-8') as f:\n",
        "    faqs = json.load(f)\n",
        "for faq in faqs:\n",
        "    text = f\"{faq['pregunta']} {faq['respuesta']}\"\n",
        "    documents.append(LlamaDocument(text=text))\n",
        "\n",
        "\n",
        "manual_docs = SimpleDirectoryReader('/content/manuales_productos').load_data()\n",
        "documents.extend(manual_docs)\n",
        "\n",
        "\n",
        "resena_docs = SimpleDirectoryReader('/content/resenas_usuarios').load_data()\n",
        "documents.extend(resena_docs)"
      ],
      "metadata": {
        "id": "CyF93xTFCcYM"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creación de las bases de datos**"
      ],
      "metadata": {
        "id": "fkg60-3auPLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base de datos 1: vectorial (ChromaDB con LlamaIndex)"
      ],
      "metadata": {
        "id": "82-K1_KLNS9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n3. Creando índice vectorial...\")\n",
        "vector_index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    show_progress=True\n",
        ")\n",
        "\n",
        "print(\"\\n Índice vectorial creado exitosamente\")\n",
        "\n",
        "vector_retriever = vector_index.as_retriever(similarity_top_k=5)\n",
        "print(\"Retriever configurado (top_k=5)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "c64beb169fed4ae4bb9d637d008e3dab",
            "fd089bfa14aa485696598bdf7fa79e7b",
            "6ec0c1575d344a358d684d6a0188ebfa",
            "68570864f650495ea866a421f571db5e",
            "a05ae6830dba4317ba4fee48c4b8ba88",
            "34099e3960424c6e92599271dda23882",
            "7c34e9e4643445b4af62df1fc8952a16",
            "899beadf482343ef80df2dc950783ce4",
            "a521b10369a64d299c437f9b86ef2da4",
            "f451686732154ec5ad625a47b2995ea5",
            "5e82cb45e0ce4372b04412c089732bdd",
            "f35f8b74579b4d60bba2bdee3f74dbb3",
            "c0a63503152746a9b5d9663bdf3348e3",
            "ba1f7b49f3a6481daae3ea01982f73d8",
            "04b3dc2d6b3d44a1a3519e07d87091b8",
            "4c1deab500a140b18ea930e7508eee56",
            "5a4cdabbbcc34147b1c610c7326f2286",
            "246c2e097ca64eb3b86bb82a8694a4d0",
            "6031c9f0dfe04cd4bcf05c7b8616b77c",
            "c2d402e163334b8bad24f2469d92ad87",
            "09a773c22dbf4f4eb5761593eef6aa3d",
            "584ba22d68324043b33d470f3d3983e2",
            "5c502951e8e34ef2bde13e51c757ce0f",
            "034da5b0d12b4736af17b48d72174160",
            "78bbd07147414f738421107303d474f5",
            "304278c9f647471382f682bc72acc3e3",
            "0d30f4b3b8de4683a6792669220e1785",
            "ec1d6d89cee3418fa4ba790674934510",
            "8e04688656384706be2718d171c729ee",
            "f68f2ff76d63432d80151a556fb334a8",
            "8e77d1c58792402ab4b3bcac00d3b768",
            "47ecebec132e4819a1e55b860d52cc5b",
            "228ff347a73b4ae984149278bfbbae47",
            "502332c6273f4563afb6a2a9ac31a706",
            "523e9a3ba2f942158e80edca4cb208b2",
            "59f4c48c8ceb4d9f8039f5662fdd5d49",
            "410822ff67d54e90889df67dd03286e6",
            "00397620d3284f2188e861c53b67850a",
            "4fcbb90025594ab7a966883210252952",
            "b49c3383e85c427b8e94d44a5467d9db",
            "ec3d72ecf9e94834bb29ccc2e3a6171f",
            "f62612ba302f42cabe84638c2f1c38c7",
            "326239c73f9b441fa58853039bdfe420",
            "2d7e3315f40c4ecc94585db12ed8388e",
            "6aa6b9bbdb814560948240703f8c6ef3",
            "54665708da524058bc874af25852f3c7",
            "c2233649cdfe4d73bd8e743d66c82061",
            "694762c150dc43a98492676bebbc9832",
            "2f2bb5f6e5714c4b95f4119ee2a10085",
            "ea559a9a06584526a0749b56b23c19a6",
            "2debcdbcb5ab42d7b5f9240a15408691",
            "ae01dcb5e6b94965a2eadaf36ed5f1a7",
            "aa7f7d6568f34b6cb28fa1bf61ec4764",
            "f3aab0613ff3400c94439fd51dc95465",
            "eca6efcf157b49efbbb48e737adba493"
          ]
        },
        "id": "pkF7ioYwzscG",
        "outputId": "5a4d7dd9-871c-4b5e-d64d-b5e55c60ea78"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Creando índice vectorial...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/8065 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c64beb169fed4ae4bb9d637d008e3dab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f35f8b74579b4d60bba2bdee3f74dbb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c502951e8e34ef2bde13e51c757ce0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "502332c6273f4563afb6a2a9ac31a706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/2023 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aa6b9bbdb814560948240703f8c6ef3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Índice vectorial creado exitosamente\n",
            "Retriever configurado (top_k=5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base de datos 2: Tabular"
      ],
      "metadata": {
        "id": "mOvEvlGvXktn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraer metadata de los dataframes (para el LLM)\n",
        "def extract_dataframe_metadata(df, df_name):\n",
        "    \"\"\"Extrae información relevante del DataFrame para el LLM\"\"\"\n",
        "    metadata = {\n",
        "        'nombre': df_name,\n",
        "        'shape': df.shape,\n",
        "        'columnas': list(df.columns),\n",
        "        'tipos': df.dtypes.to_dict(),\n",
        "        'info_columnas': {}\n",
        "    }\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_info = {}\n",
        "\n",
        "        if df[col].dtype in ['object', 'category']:\n",
        "            #Campos categóricos\n",
        "            valores_unicos = df[col].dropna().unique()\n",
        "            col_info['tipo'] = 'categórico'\n",
        "            col_info['valores_unicos'] = list(valores_unicos[:20])  # Máximo 20\n",
        "            col_info['n_unicos'] = len(valores_unicos)\n",
        "        elif df[col].dtype in ['int64', 'float64']:\n",
        "            #Campos numéricos\n",
        "            col_info['tipo'] = 'numérico'\n",
        "            col_info['min'] = float(df[col].min()) if not pd.isna(df[col].min()) else None\n",
        "            col_info['max'] = float(df[col].max()) if not pd.isna(df[col].max()) else None\n",
        "            col_info['mean'] = float(df[col].mean()) if not pd.isna(df[col].mean()) else None\n",
        "        else:\n",
        "            col_info['tipo'] = str(df[col].dtype)\n",
        "\n",
        "        metadata['info_columnas'][col] = col_info\n",
        "\n",
        "    return metadata\n",
        "\n",
        "#Extraer metadata\n",
        "metadata_productos = extract_dataframe_metadata(df_productos, 'productos')\n",
        "metadata_ventas = extract_dataframe_metadata(df_ventas, 'ventas_historicas')\n",
        "metadata_inventario = extract_dataframe_metadata(df_inventario, 'inventario_sucursales')\n",
        "\n",
        "#Mostrar ejemplo de metadata\n",
        "print(\"\\n   Ejemplo - Metadata de productos:\")\n",
        "print(f\"   - Shape: {metadata_productos['shape']}\")\n",
        "print(f\"   - Columnas: {metadata_productos['columnas'][:5]}...\")\n",
        "print(f\"   - Categorías únicas: {metadata_productos['info_columnas']['categoria']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZmyu5tUXoIa",
        "outputId": "ef13180a-c90c-4f6f-cb91-079f31fbfaa8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   Ejemplo - Metadata de productos:\n",
            "   - Shape: (300, 14)\n",
            "   - Columnas: ['id_producto', 'nombre', 'categoria', 'subcategoria', 'marca']...\n",
            "   - Categorías únicas: {'tipo': 'categórico', 'valores_unicos': ['Cocina', 'Climatización', 'Lavado', 'Audio y Video'], 'n_unicos': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CREANDO BASE DE DATOS VECTORIAL\")\n",
        "documents = []\n",
        "\n",
        "#Convertir FAQs a documentos\n",
        "print(\"\\n1. Procesando FAQs...\")\n",
        "for faq in faqs:\n",
        "    texto = f\"Pregunta: {faq['pregunta']}\\nRespuesta: {faq['respuesta']}\"\n",
        "\n",
        "    #Crear documento de LlamaIndex con metadata\n",
        "    doc = LlamaDocument(\n",
        "        text=texto,\n",
        "        metadata={\n",
        "            'id_faq': faq['id_faq'],\n",
        "            'id_producto': faq['id_producto'],\n",
        "            'nombre_producto': faq['nombre_producto'],\n",
        "            'categoria': faq['categoria'],\n",
        "            'tipo': 'faq'\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f\"    {len(faqs)} FAQs convertidos a documentos\")\n",
        "\n",
        "#Convertir Manuales a documentos\n",
        "print(\"\\n2. Procesando manuales...\")\n",
        "for manual in manuales:\n",
        "    doc = LlamaDocument(\n",
        "        text=manual['contenido'],\n",
        "        metadata={\n",
        "            'id_producto': manual['id_producto'],\n",
        "            'nombre_archivo': manual['nombre_archivo'],\n",
        "            'tipo': 'manual'\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f\"    {len(manuales)} manuales convertidos a documentos\")\n",
        "\n",
        "print(f\"\\n Total documentos para indexar: {len(documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py1pgG1PNSyf",
        "outputId": "c993177a-45e9-41c9-f07e-20ce18e8664f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREANDO BASE DE DATOS VECTORIAL\n",
            "\n",
            "1. Procesando FAQs...\n",
            "    3000 FAQs convertidos a documentos\n",
            "\n",
            "2. Procesando manuales...\n",
            "    50 manuales convertidos a documentos\n",
            "\n",
            " Total documentos para indexar: 3050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Crear string de información para el LLM\n",
        "def create_metadata_string(metadata):\n",
        "    \"\"\"Convierte metadata en string legible para el LLM\"\"\"\n",
        "    lines = []\n",
        "    lines.append(f\"DataFrame: {metadata['nombre']}\")\n",
        "    lines.append(f\"Shape: {metadata['shape'][0]} filas, {metadata['shape'][1]} columnas\")\n",
        "    lines.append(f\"Columnas: {', '.join(metadata['columnas'])}\")\n",
        "    lines.append(\"\\nInformación de columnas:\")\n",
        "\n",
        "    for col, info in metadata['info_columnas'].items():\n",
        "        if info['tipo'] == 'categórico':\n",
        "            valores = info['valores_unicos'][:10]\n",
        "            lines.append(f\"  - {col}: categórico, valores ejemplo: {valores}\")\n",
        "        elif info['tipo'] == 'numérico':\n",
        "            lines.append(f\"  - {col}: numérico, rango [{info['min']}, {info['max']}]\")\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "info_productos = create_metadata_string(metadata_productos)\n",
        "info_ventas = create_metadata_string(metadata_ventas)\n",
        "info_inventario = create_metadata_string(metadata_inventario)"
      ],
      "metadata": {
        "id": "TB8KO1bWXquv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base de datos 3: Grafos (Neo4j local)\n"
      ],
      "metadata": {
        "id": "T5v7S_aXYFAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt-get install wget -y\n",
        "!wget -O - https://debian.neo4j.com/neotechnology.gpg.key | apt-key add -\n",
        "!echo 'deb https://debian.neo4j.com stable latest' | tee -a /etc/apt/sources.list.d/neo4j.list\n",
        "!apt-get update\n",
        "!apt-get install neo4j -y"
      ],
      "metadata": {
        "id": "IlRB_JL6YKTA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Quitar autenticación\n",
        "!sed -i 's/#dbms.security.auth_enabled=false/dbms.security.auth_enabled=false/' /etc/neo4j/neo4j.conf\n",
        "!service neo4j start\n",
        "time.sleep(3)\n",
        "\n",
        "uri = \"neo4j://localhost:7687\"\n",
        "driver = GraphDatabase.driver(uri)\n",
        "print(\"Neo4j iniciado y conectado\")\n",
        "\n",
        "#Verifico si está vacío\n",
        "with driver.session() as session:\n",
        "    result = session.run(\"MATCH (n) RETURN count(n) as total\")\n",
        "    total = result.single()[\"total\"]\n",
        "    print(f\"Nodos actuales: {total}\")\n",
        "\n",
        "#Poblar si está vacío\n",
        "if total == 0:\n",
        "\n",
        "    relaciones = []\n",
        "\n",
        "    for _, row in df_productos.iterrows():\n",
        "        relaciones.append({\n",
        "            'source': row['id_producto'], 'source_type': 'Producto',\n",
        "            'relation': 'PERTENECE_A',\n",
        "            'target': row['categoria'], 'target_type': 'Categoria'\n",
        "        })\n",
        "        relaciones.append({\n",
        "            'source': row['categoria'], 'source_type': 'Categoria',\n",
        "            'relation': 'TIENE_SUBCATEGORIA',\n",
        "            'target': row['subcategoria'], 'target_type': 'Subcategoria'\n",
        "        })\n",
        "        relaciones.append({\n",
        "            'source': row['id_producto'], 'source_type': 'Producto',\n",
        "            'relation': 'ES_DE_MARCA',\n",
        "            'target': row['marca'], 'target_type': 'Marca'\n",
        "        })\n",
        "\n",
        "    for _, row in df_ventas.head(100).iterrows():\n",
        "        relaciones.append({\n",
        "            'source': row['id_producto'], 'source_type': 'Producto',\n",
        "            'relation': 'VENDIDO_POR',\n",
        "            'target': row['id_vendedor'], 'target_type': 'Vendedor'\n",
        "        })\n",
        "        relaciones.append({\n",
        "            'source': row['id_vendedor'], 'source_type': 'Vendedor',\n",
        "            'relation': 'TRABAJA_EN',\n",
        "            'target': row['sucursal'], 'target_type': 'Sucursal'\n",
        "        })\n",
        "\n",
        "    df_relaciones = pd.DataFrame(relaciones).drop_duplicates()\n",
        "    print(f\"Total relaciones: {len(df_relaciones)}\")\n",
        "\n",
        "    #Funciones para Neo4j\n",
        "    def clear_database(tx):\n",
        "        tx.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "    def create_schema(tx):\n",
        "        tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE\")\n",
        "        tx.run(\"CREATE INDEX IF NOT EXISTS FOR (e:Entity) ON (e.name)\")\n",
        "\n",
        "    def add_triple(tx, subject, relation, obj, subject_type, object_type):\n",
        "        query = f\"\"\"\n",
        "        MERGE (s:Entity:{subject_type} {{name: $subject}})\n",
        "        MERGE (o:Entity:{object_type} {{name: $object}})\n",
        "        MERGE (s)-[r:`{relation}`]->(o)\n",
        "        \"\"\"\n",
        "        tx.run(query, subject=subject, object=obj)\n",
        "\n",
        "    print(\"\\nInsertando en Neo4j...\")\n",
        "    with driver.session() as session:\n",
        "        session.execute_write(clear_database)\n",
        "        session.execute_write(create_schema)\n",
        "\n",
        "    with driver.session() as session:\n",
        "        for i, row in df_relaciones.iterrows():\n",
        "            session.execute_write(\n",
        "                add_triple,\n",
        "                row['source'], row['relation'], row['target'],\n",
        "                row['source_type'], row['target_type']\n",
        "            )\n",
        "            if (i + 1) % 200 == 0:\n",
        "                print(f\"  Insertadas {i + 1}/{len(df_relaciones)}...\")\n",
        "\n",
        "    #Verifición\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"MATCH (n) RETURN count(n) as total\")\n",
        "        total = result.single()[\"total\"]\n",
        "        print(f\"\\n Neo4j poblado: {total} nodos\")\n",
        "else:\n",
        "    print(\"Todo correcto, Neo4j ya tiene datos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPv-WGMMYTxh",
        "outputId": "54eeca7a-f707-42e4-abd8-dd408f385786"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neo4j is already running (pid:10084).\n",
            "Run with '--verbose' for a more detailed error message.\n",
            "Neo4j iniciado y conectado\n",
            "Nodos actuales: 410\n",
            "Todo correcto, Neo4j ya tiene datos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Funciones de búsqueda**"
      ],
      "metadata": {
        "id": "T1xFOcJwuc8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BÚSQUEDA HÍBRIDA: BM25 + Reranking"
      ],
      "metadata": {
        "id": "R5H-7FI5ulJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CONFIGURANDO BÚSQUEDA HÍBRIDA (BM25 + RERANKING)\")\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "print(\"\\n1. Implementando BM25Searcher...\")\n",
        "\n",
        "class BM25Searcher:\n",
        "    \"\"\"\n",
        "    Implementación de búsqueda BM25 compatible con LlamaIndex (siguiendo clase)\n",
        "    \"\"\"\n",
        "    def __init__(self, documents: List[LlamaDocument], language: str = 'spanish'):\n",
        "        self.language = language\n",
        "        self.documents = documents\n",
        "\n",
        "        self.text_nodes = []\n",
        "        for i, doc in enumerate(self.documents):\n",
        "            text_node = TextNode(\n",
        "                text=doc.text,\n",
        "                metadata=doc.metadata if hasattr(doc, 'metadata') else {},\n",
        "                id_=f\"bm25_node_{i}\"\n",
        "            )\n",
        "            self.text_nodes.append(text_node)\n",
        "\n",
        "        self.corpus = [self._preprocess_text(node.text) for node in self.text_nodes]\n",
        "        self.tokenized_corpus = [self._tokenize(text) for text in self.corpus]\n",
        "\n",
        "        # Inicializa BM25\n",
        "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
        "\n",
        "    def _preprocess_text(self, text: str) -> str:\n",
        "        text = text.lower()\n",
        "        text = unidecode(text)\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        return word_tokenize(text, language=self.language)\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 5) -> List[NodeWithScore]:\n",
        "        processed_query = self._preprocess_text(query)\n",
        "        tokenized_query = self._tokenize(processed_query)\n",
        "\n",
        "        scores = self.bm25.get_scores(tokenized_query)\n",
        "        top_indices = np.argsort(scores)[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if scores[idx] > 0:\n",
        "                node = NodeWithScore(\n",
        "                    node=self.text_nodes[idx],\n",
        "                    score=float(scores[idx])\n",
        "                )\n",
        "                results.append(node)\n",
        "\n",
        "        return results\n",
        "\n",
        "bm25_searcher = BM25Searcher(documents=documents)\n",
        "print(\"    BM25Searcher creado\")\n",
        "\n",
        "class Reranker:\n",
        "    \"\"\"Reranker con CrossEncoder\"\"\"\n",
        "    def __init__(self, model_name: str = 'cross-encoder/ms-marco-MiniLM-L-6-v2'):\n",
        "        self.model = CrossEncoder(model_name)\n",
        "\n",
        "    def rerank(self, query: str, nodes: List[NodeWithScore], top_k: int = None) -> List[NodeWithScore]:\n",
        "        if not nodes:\n",
        "            return nodes\n",
        "\n",
        "        pairs = [(query, node.node.text) for node in nodes]\n",
        "\n",
        "        scores = self.model.predict(pairs)\n",
        "\n",
        "        scored_nodes = []\n",
        "        for score, node in zip(scores, nodes):\n",
        "            node.score = float(score)\n",
        "            scored_nodes.append(node)\n",
        "        scored_nodes.sort(key=lambda x: x.score, reverse=True)\n",
        "\n",
        "        if top_k:\n",
        "            scored_nodes = scored_nodes[:top_k]\n",
        "\n",
        "        return scored_nodes\n",
        "\n",
        "#Crear reranker\n",
        "print(\"   Cargando modelo de reranking...\")\n",
        "reranker = Reranker()\n",
        "print(\"    Reranker creado\")\n",
        "\n",
        "print(\"\\n Búsqueda híbrida configurada correctamente\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPpLvyuDukMx",
        "outputId": "54264e30-1c84-434d-caeb-cc28c8afa860"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIGURANDO BÚSQUEDA HÍBRIDA (BM25 + RERANKING)\n",
            "\n",
            "1. Implementando BM25Searcher...\n",
            "    BM25Searcher creado\n",
            "   Cargando modelo de reranking...\n",
            "    Reranker creado\n",
            "\n",
            " Búsqueda híbrida configurada correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FUNCIÓN DE BÚSQUEDA HÍBRIDA (Vector + BM25 + Rerank)"
      ],
      "metadata": {
        "id": "2PKULyAtvATt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_search(query: str, top_k: int = 3, apply_rerank: bool = True):\n",
        "    \"\"\"\n",
        "    Búsqueda híbrida que combina:\n",
        "    1. Búsqueda vectorial (semántica)\n",
        "    2. Búsqueda BM25 (palabras clave)\n",
        "    3. Reranking (opcional)\n",
        "\n",
        "    Args:\n",
        "        query: Consulta del usuario\n",
        "        top_k: Número de resultados finales\n",
        "        apply_rerank: Si aplicar reranking o no\n",
        "\n",
        "    Returns:\n",
        "        Lista de NodeWithScore con los mejores resultados\n",
        "    \"\"\"\n",
        "    #Búsqueda vectorial\n",
        "    vector_nodes = vector_retriever.retrieve(query)\n",
        "\n",
        "    #Búsqueda BM25\n",
        "    bm25_nodes = bm25_searcher.retrieve(query, top_k=5)\n",
        "\n",
        "    #Combinar resultados\n",
        "    all_nodes = vector_nodes.copy()\n",
        "\n",
        "    for bm25_node in bm25_nodes:\n",
        "        is_duplicate = any(\n",
        "            bm25_node.node.text == vector_node.node.text\n",
        "            for vector_node in vector_nodes\n",
        "        )\n",
        "        if not is_duplicate:\n",
        "            all_nodes.append(bm25_node)\n",
        "\n",
        "    #Aplico reranking\n",
        "    if apply_rerank:\n",
        "        final_nodes = reranker.rerank(query, all_nodes, top_k=top_k)\n",
        "    else:\n",
        "        final_nodes = all_nodes[:top_k]\n",
        "\n",
        "    return final_nodes\n",
        "\n",
        "print(\"Función hybrid_search creada\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zUO7rh2u9oW",
        "outputId": "812969bf-0b45-410b-d2f9-7e97172103c3"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función hybrid_search creada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Clasificador de Intenciones**"
      ],
      "metadata": {
        "id": "KrC0GQTzfXJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3 intenciones posibles\n",
        "INTENCIONES = ['vectorial', 'tabular', 'grafos']\n",
        "\n",
        "print(\"\\nIntenciones del sistema:\")\n",
        "print(\"  1. vectorial: Preguntas sobre uso, FAQs, manuales\")\n",
        "print(\"  2. tabular: Preguntas sobre precios, stock, especificaciones\")\n",
        "print(\"  3. grafos: Preguntas sobre relaciones (categorías, marcas, sucursales)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwpGko7rfYLK",
        "outputId": "e00c1da6-4c44-486c-dcac-d5c7c65dc436"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Intenciones del sistema:\n",
            "  1. vectorial: Preguntas sobre uso, FAQs, manuales\n",
            "  2. tabular: Preguntas sobre precios, stock, especificaciones\n",
            "  3. grafos: Preguntas sobre relaciones (categorías, marcas, sucursales)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de datos sintéticos para entrenar los clasificadores"
      ],
      "metadata": {
        "id": "BfEcmU-wqbGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datos_entrenamiento = []\n",
        "\n",
        "#VECTORIAL - Preguntas sobre uso, FAQs, manuales\n",
        "preguntas_vectorial = [\n",
        "    \"¿Cómo uso mi licuadora para hacer smoothies?\",\n",
        "    \"¿Qué voltaje requiere el rallador digital?\",\n",
        "    \"¿Cómo se limpia la freidora?\",\n",
        "    \"¿Para qué sirve el botón pulse de la licuadora?\",\n",
        "    \"¿Qué dice el manual sobre el mantenimiento?\",\n",
        "    \"¿Cómo funciona la función de auto-limpieza?\",\n",
        "    \"¿Qué especificaciones técnicas tiene este producto?\",\n",
        "    \"¿Cómo instalo el aire acondicionado?\",\n",
        "    \"¿Qué precauciones debo tener al usar la batidora?\",\n",
        "    \"¿Cómo se programa el microondas?\",\n",
        "    \"Necesito instrucciones de uso del producto\",\n",
        "    \"¿Qué dice la garantía sobre este problema?\",\n",
        "    \"¿Cómo se usa correctamente este electrodoméstico?\",\n",
        "    \"¿Qué opinan los usuarios sobre esta cafetera?\",\n",
        "    \"¿Tiene buenas reseñas este producto?\",\n",
        "    \"¿Qué dicen los clientes sobre la calidad?\",\n",
        "    \"¿Es recomendable según las opiniones?\",\n",
        "    \"¿Cuál es la valoración promedio?\",\n",
        "    \"¿Qué comentarios tienen los usuarios?\",\n",
        "    \"¿Alguien probó este modelo?\",\n",
        "]\n",
        "\n",
        "#TABULAR - Preguntas sobre precios, stock, especificaciones numéricas\n",
        "preguntas_tabular = [\n",
        "    \"¿Cuáles son las licuadoras de menos de $200?\",\n",
        "    \"¿Qué productos cuestan entre $100 y $500?\",\n",
        "    \"¿Cuánto stock hay de este producto?\",\n",
        "    \"¿Qué productos están en la categoría Cocina?\",\n",
        "    \"Quiero una licuadora con buenas reseñas y precio bajo\",\n",
        "    \"¿Cuál es el precio de la licuadora TechHome?\",\n",
        "    \"¿Hay stock disponible en Buenos Aires?\",\n",
        "    \"Necesito productos de color blanco\",\n",
        "    \"¿Qué electrodomésticos tienen garantía de 36 meses?\",\n",
        "    \"Busco productos con potencia mayor a 1000W\",\n",
        "    \"¿Cuál es la licuadora más barata?\",\n",
        "    \"¿Qué productos están en oferta?\",\n",
        "    \"¿Cuántos productos hay en stock?\",\n",
        "    \"Quiero ver todos los productos de la marca ChefMaster\",\n",
        "    \"¿Qué heladeras tienen capacidad mayor a 300L?\",\n",
        "    \"Necesito un producto con peso menor a 5kg\",\n",
        "    \"¿Cuáles son los productos más vendidos?\",\n",
        "    \"¿Qué productos tienen descuento?\",\n",
        "    \"Busco electrodomésticos de 220V\",\n",
        "    \"¿Cuál es el rango de precios de licuadoras?\",\n",
        "]\n",
        "\n",
        "#GRAFOS - Preguntas sobre relaciones, categorías, conexiones\n",
        "preguntas_grafos = [\n",
        "    \"¿Qué productos están relacionados con la categoría Cocina?\",\n",
        "    \"¿Qué vendedores venden productos de la marca TechHome?\",\n",
        "    \"¿En qué sucursales se vende este producto?\",\n",
        "    \"¿Qué marcas tienen productos en Climatización?\",\n",
        "    \"¿Qué subcategorías tiene la categoría Cocina?\",\n",
        "    \"¿Qué productos vende el vendedor V0001?\",\n",
        "    \"¿Qué relación hay entre esta marca y esa categoría?\",\n",
        "    \"¿Qué sucursales venden productos ChefMaster?\",\n",
        "    \"¿Qué categorías de productos hay?\",\n",
        "    \"¿Qué marcas están disponibles?\",\n",
        "    \"¿Qué vendedores trabajan en Córdoba?\",\n",
        "    \"¿Qué productos están conectados con Audio y Video?\",\n",
        "    \"¿Qué marcas tienen productos en múltiples categorías?\",\n",
        "    \"¿Cuántas subcategorías tiene cada categoría?\",\n",
        "    \"¿Qué vendedores venden productos de Preparación?\",\n",
        "    \"¿En cuántas sucursales está disponible la marca KitchenPro?\",\n",
        "    \"¿Qué productos comparten categoría con la licuadora?\",\n",
        "    \"¿Qué conexiones hay entre vendedores y productos?\",\n",
        "    \"¿Qué estructura tienen las categorías?\",\n",
        "    \"¿Qué productos pertenecen a la subcategoría Refrigeración?\",\n",
        "]\n",
        "\n",
        "#Creación del dataset\n",
        "for pregunta in preguntas_vectorial:\n",
        "    datos_entrenamiento.append({'texto': pregunta, 'intencion': 'vectorial'})\n",
        "\n",
        "for pregunta in preguntas_tabular:\n",
        "    datos_entrenamiento.append({'texto': pregunta, 'intencion': 'tabular'})\n",
        "\n",
        "for pregunta in preguntas_grafos:\n",
        "    datos_entrenamiento.append({'texto': pregunta, 'intencion': 'grafos'})\n",
        "\n",
        "df_train = pd.DataFrame(datos_entrenamiento)\n",
        "\n",
        "print(f\"   Distribución:\")\n",
        "print(df_train['intencion'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lyiCLb3fbI0",
        "outputId": "4660d135-03b6-48d0-d4e1-6ac942730823"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Distribución:\n",
            "intencion\n",
            "vectorial    20\n",
            "tabular      20\n",
            "grafos       20\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento de clasificador basado en embeddings"
      ],
      "metadata": {
        "id": "GBem6uUSfrDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_train['texto'].values,\n",
        "    df_train['intencion'].values,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=df_train['intencion'].values\n",
        ")\n",
        "\n",
        "\n",
        "#Generar embeddings usando el modelo configurado\n",
        "train_embeddings = [Settings.embed_model.get_text_embedding(text) for text in X_train]\n",
        "test_embeddings = [Settings.embed_model.get_text_embedding(text) for text in X_test]\n",
        "\n",
        "train_embeddings = np.array(train_embeddings)\n",
        "test_embeddings = np.array(test_embeddings)\n",
        "\n",
        "#Entrenar clasificador\n",
        "clf_trained = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_trained.fit(train_embeddings, y_train)\n",
        "\n",
        "#Predecir\n",
        "y_pred_trained = clf_trained.predict(test_embeddings)\n",
        "\n",
        "#Métricas\n",
        "accuracy_trained = accuracy_score(y_test, y_pred_trained)\n",
        "print(f\"   Accuracy: {accuracy_trained:.3f}\")\n",
        "\n",
        "print(\"\\n   Reporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred_trained, target_names=INTENCIONES))\n",
        "\n",
        "#Matriz de confusión\n",
        "print(\"\\n   Matriz de confusión:\")\n",
        "cm_trained = confusion_matrix(y_test, y_pred_trained, labels=INTENCIONES)\n",
        "print(cm_trained)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBwBt2A5fqb9",
        "outputId": "4faf0951-e461-4126-b903-67bab7790b13"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Accuracy: 0.722\n",
            "\n",
            "   Reporte de clasificación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   vectorial       0.83      0.83      0.83         6\n",
            "     tabular       0.60      0.50      0.55         6\n",
            "      grafos       0.71      0.83      0.77         6\n",
            "\n",
            "    accuracy                           0.72        18\n",
            "   macro avg       0.72      0.72      0.72        18\n",
            "weighted avg       0.72      0.72      0.72        18\n",
            "\n",
            "\n",
            "   Matriz de confusión:\n",
            "[[5 1 0]\n",
            " [2 3 1]\n",
            " [0 1 5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entrenamiento de clasificador basado en LLM"
      ],
      "metadata": {
        "id": "vepJ9TW9rNhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMFewShotClassifier:\n",
        "    \"\"\"\n",
        "    Clasificador que usa Few-Shot Learning con embeddings\n",
        "    Simula el comportamiento de un LLM clasificando por similitud\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_model):\n",
        "        self.embed_model = embed_model\n",
        "        self.examples = {\n",
        "            'vectorial': [\n",
        "                \"¿Cómo uso mi licuadora?\",\n",
        "                \"¿Qué dice el manual sobre mantenimiento?\",\n",
        "                \"¿Qué opinan los usuarios sobre este producto?\"\n",
        "            ],\n",
        "            'tabular': [\n",
        "                \"¿Cuáles son las licuadoras de menos de $200?\",\n",
        "                \"¿Cuánto stock hay disponible?\",\n",
        "                \"Quiero productos de la marca ChefMaster\"\n",
        "            ],\n",
        "            'grafos': [\n",
        "                \"¿Qué productos están en la categoría Cocina?\",\n",
        "                \"¿Qué vendedores venden productos TechHome?\",\n",
        "                \"¿En qué sucursales se vende este producto?\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        self.example_embeddings = {}\n",
        "        for intent, examples in self.examples.items():\n",
        "            embeddings = [embed_model.get_text_embedding(ex) for ex in examples]\n",
        "            self.example_embeddings[intent] = np.array(embeddings)\n",
        "\n",
        "    def predict(self, query: str) -> str:\n",
        "        \"\"\"Clasifica la query por similitud con ejemplos\"\"\"\n",
        "        query_embedding = self.embed_model.get_text_embedding(query)\n",
        "        query_embedding = np.array(query_embedding)\n",
        "\n",
        "        #Calcular similitud con cada intención\n",
        "        similarities = {}\n",
        "        for intent, examples_emb in self.example_embeddings.items():\n",
        "            sims = np.dot(examples_emb, query_embedding) / (\n",
        "                np.linalg.norm(examples_emb, axis=1) * np.linalg.norm(query_embedding)\n",
        "            )\n",
        "            similarities[intent] = np.mean(sims)\n",
        "\n",
        "        # Retornar intención con mayor similitud\n",
        "        return max(similarities, key=similarities.get)\n",
        "\n",
        "#Crear clasificador\n",
        "clf_llm = LLMFewShotClassifier(Settings.embed_model)\n",
        "\n",
        "#test\n",
        "y_pred_llm = [clf_llm.predict(text) for text in X_test]\n",
        "\n",
        "accuracy_llm = accuracy_score(y_test, y_pred_llm)\n",
        "print(f\"   Accuracy: {accuracy_llm:.3f}\")\n",
        "\n",
        "print(\"\\n   Reporte de clasificación:\")\n",
        "print(classification_report(y_test, y_pred_llm, target_names=INTENCIONES))\n",
        "\n",
        "#Matriz de confusión\n",
        "print(\"\\n   Matriz de confusión:\")\n",
        "cm_llm = confusion_matrix(y_test, y_pred_llm, labels=INTENCIONES)\n",
        "print(cm_llm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxrKV2AsfuYa",
        "outputId": "2493ed11-3393-488d-8557-5985b05231db"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Accuracy: 0.722\n",
            "\n",
            "   Reporte de clasificación:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   vectorial       0.67      1.00      0.80         6\n",
            "     tabular       0.75      0.50      0.60         6\n",
            "      grafos       0.80      0.67      0.73         6\n",
            "\n",
            "    accuracy                           0.72        18\n",
            "   macro avg       0.74      0.72      0.71        18\n",
            "weighted avg       0.74      0.72      0.71        18\n",
            "\n",
            "\n",
            "   Matriz de confusión:\n",
            "[[4 1 1]\n",
            " [1 3 2]\n",
            " [0 0 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ambos clasificadores tuvieron Accuracy = 72% por lo que voy a hacer la comparación entre ambos modelos\n"
      ],
      "metadata": {
        "id": "gK3pqsbZf-Zh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparación de los clasificadores"
      ],
      "metadata": {
        "id": "CdJ3Qdm9qMYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"COMPARACIÓN DE CLASIFICADORES\")\n",
        "\n",
        "#Comparación de métricas\n",
        "comparacion = pd.DataFrame({\n",
        "    'Métrica': ['Accuracy', 'Precision (macro)', 'Recall (macro)', 'F1-Score (macro)'],\n",
        "    'Clasificador Entrenado': [\n",
        "        accuracy_trained,\n",
        "        0.72,\n",
        "        0.72,\n",
        "        0.72\n",
        "    ],\n",
        "    'Clasificador LLM Few-Shot': [\n",
        "        accuracy_llm,\n",
        "        0.74,\n",
        "        0.72,\n",
        "        0.71\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\", comparacion.to_string(index=False))\n",
        "\n",
        "clasificador = clf_llm\n",
        "print(\"\\n Clasificador final seleccionado: LLM Few-Shot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhAftuGLgInO",
        "outputId": "bddabc76-13ed-4be0-85f2-ffee9f1bbec1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPARACIÓN DE CLASIFICADORES\n",
            "\n",
            "           Métrica  Clasificador Entrenado  Clasificador LLM Few-Shot\n",
            "         Accuracy                0.722222                   0.722222\n",
            "Precision (macro)                0.720000                   0.740000\n",
            "   Recall (macro)                0.720000                   0.720000\n",
            " F1-Score (macro)                0.720000                   0.710000\n",
            "\n",
            " Clasificador final seleccionado: LLM Few-Shot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creación Wrapper, Pipeline y RAG**"
      ],
      "metadata": {
        "id": "7VVPkrzkyCNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función wrapper para usar en el pipeline\n",
        "def classify_intent(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Clasifica la intención de una consulta del usuario\n",
        "\n",
        "    Args:\n",
        "        query: Consulta del usuario\n",
        "\n",
        "    Returns:\n",
        "        'vectorial', 'tabular', o 'grafos'\n",
        "    \"\"\"\n",
        "    return clasificador.predict(query)\n",
        "\n",
        "print(\"\\n Función classify_intent() creada\")\n",
        "\n",
        "# Pruebas\n",
        "print(\"PRUEBAS DEL CLASIFICADOR\")\n",
        "pruebas = [\n",
        "    \"¿Cómo uso mi licuadora para hacer smoothies?\",\n",
        "    \"¿Cuáles son las licuadoras de menos de $300?\",\n",
        "    \"¿Qué productos están relacionados con Cocina?\",\n",
        "    \"¿Qué opinan los usuarios sobre esta cafetera?\",\n",
        "    \"¿Cuánto stock hay disponible?\",\n",
        "    \"¿Qué marcas tienen productos en Climatización?\"\n",
        "]\n",
        "\n",
        "for consulta in pruebas:\n",
        "    intencion = classify_intent(consulta)\n",
        "    print(f\"\\n'{consulta}'\")\n",
        "    print(f\"  → Intención: {intencion}\")\n",
        "\n",
        "print(\"\\nClasificador de intenciones funcionando\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb_DlzZXgLSr",
        "outputId": "3bb46275-f9f7-4fc1-f019-7b4cfb3929dd"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Función classify_intent() creada\n",
            "PRUEBAS DEL CLASIFICADOR\n",
            "\n",
            "'¿Cómo uso mi licuadora para hacer smoothies?'\n",
            "  → Intención: vectorial\n",
            "\n",
            "'¿Cuáles son las licuadoras de menos de $300?'\n",
            "  → Intención: tabular\n",
            "\n",
            "'¿Qué productos están relacionados con Cocina?'\n",
            "  → Intención: grafos\n",
            "\n",
            "'¿Qué opinan los usuarios sobre esta cafetera?'\n",
            "  → Intención: vectorial\n",
            "\n",
            "'¿Cuánto stock hay disponible?'\n",
            "  → Intención: tabular\n",
            "\n",
            "'¿Qué marcas tienen productos en Climatización?'\n",
            "  → Intención: grafos\n",
            "\n",
            "Clasificador de intenciones funcionando\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline de RAG"
      ],
      "metadata": {
        "id": "Y7si-f_5-mtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CREANDO PIPELINE DE RAG\")\n",
        "\n",
        "# JUSTIFICACIÓN DEL LLM\n",
        "justificacion_llm = \"\"\"\n",
        "JUSTIFICACIÓN DEL LLM:\n",
        "\n",
        "Ubicación: API en la nube (Gemini Flash)\n",
        "Modelo: gemini-1.5-flash\n",
        "Razón de elección:\n",
        "  1. Es GRATUITO con límite generoso (15 requests/minuto)\n",
        "  2. Multilingüe (soporta español nativamente)\n",
        "  3. Contexto de 1M tokens (suficiente para los chunks trabajados)\n",
        "  4. Baja latencia (~1-2 segundos de respuesta)\n",
        "\n",
        "Alternativas consideradas:\n",
        "  - GPT-4: Muy costoso para este TP\n",
        "  - Llama local: Requiere GPU y es más lento\n",
        "  - Claude: Requiere API key de pago\n",
        "\"\"\"\n",
        "\n",
        "print(justificacion_llm)\n",
        "\n",
        "#Mock LLM para testing\n",
        "class MockLLM:\n",
        "    \"\"\"LLM simulado para testing sin API key\"\"\"\n",
        "    def generate(self, prompt: str) -> str:\n",
        "        if \"Pregunta:\" in prompt:\n",
        "            pregunta = prompt.split(\"Pregunta:\")[-1].strip()\n",
        "        else:\n",
        "            pregunta = prompt[:100]\n",
        "\n",
        "        if \"Contexto:\" in prompt and prompt.split(\"Contexto:\")[1].strip():\n",
        "            return f\"Basándome en la información disponible, puedo responder a tu consulta sobre: {pregunta[:50]}... La documentación indica detalles relevantes que responden tu pregunta.\"\n",
        "        else:\n",
        "            return \"No encontré información específica en la base de datos. ¿Podrías reformular tu pregunta o proporcionar más detalles?\"\n",
        "\n",
        "llm = MockLLM()\n",
        "print(\"\\n LLM configurado\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDa2ARvEgZPV",
        "outputId": "ae36438d-d900-451d-ed07-7b4f5afe4e16"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREANDO PIPELINE DE RAG\n",
            "\n",
            "JUSTIFICACIÓN DEL LLM:\n",
            "\n",
            "Ubicación: API en la nube (Gemini Flash)\n",
            "Modelo: gemini-1.5-flash\n",
            "Razón de elección:\n",
            "  1. Es GRATUITO con límite generoso (15 requests/minuto)\n",
            "  2. Multilingüe (soporta español nativamente)\n",
            "  3. Contexto de 1M tokens (suficiente para los chunks trabajados)\n",
            "  4. Baja latencia (~1-2 segundos de respuesta)\n",
            "\n",
            "Alternativas consideradas:\n",
            "  - GPT-4: Muy costoso para este TP\n",
            "  - Llama local: Requiere GPU y es más lento\n",
            "  - Claude: Requiere API key de pago\n",
            "\n",
            "\n",
            " LLM configurado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de sistema RAG"
      ],
      "metadata": {
        "id": "TnCjIBFEocqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        self.conversation_history = []\n",
        "\n",
        "    def retrieve(self, query: str, intent: str):\n",
        "        \"\"\"Recupera información según la intención\"\"\"\n",
        "\n",
        "        if intent == 'vectorial':\n",
        "            #Búsqueda híbrida\n",
        "            results = hybrid_search(query, top_k=3, apply_rerank=True)\n",
        "            context = \"\\n\\n\".join([\n",
        "                f\"[Documento {i+1}]\\n{node.node.text[:500]}...\"\n",
        "                for i, node in enumerate(results)\n",
        "            ])\n",
        "            return context, \"búsqueda semántica\"\n",
        "\n",
        "        elif intent == 'tabular':\n",
        "            context = f\"Información de productos disponible en la base de datos tabular con {len(df_productos)} productos.\"\n",
        "            return context, \"base de datos tabular\"\n",
        "\n",
        "        elif intent == 'grafos':\n",
        "            context = \"Información de relaciones disponible en el grafo de conocimiento.\"\n",
        "            return context, \"grafo de conocimiento\"\n",
        "\n",
        "        return \"\", \"ninguna fuente\"\n",
        "\n",
        "    def generate_response(self, query: str) -> dict:\n",
        "        \"\"\"\n",
        "        Genera respuesta completa para una consulta\n",
        "\n",
        "        Returns:\n",
        "            dict con 'respuesta', 'fuente', 'intencion'\n",
        "        \"\"\"\n",
        "        #Clasificar intención\n",
        "        intent = classify_intent(query)\n",
        "\n",
        "        #Recuperar contexto\n",
        "        context, source = self.retrieve(query, intent)\n",
        "\n",
        "        #Construir prompt\n",
        "        prompt = f\"\"\"Contexto: {context}\n",
        "\n",
        "Pregunta: {query}\n",
        "\n",
        "Instrucciones:\n",
        "- Responde en español\n",
        "- Usa solo la información del contexto\n",
        "- Si no hay información suficiente, indica que no puedes responder\n",
        "- Sé claro y conciso\n",
        "\n",
        "Respuesta:\"\"\"\n",
        "\n",
        "        #Generar respuesta\n",
        "        response = llm.generate(prompt)\n",
        "\n",
        "        #Guardar en memoria\n",
        "        self.conversation_history.append({\n",
        "            'query': query,\n",
        "            'intent': intent,\n",
        "            'response': response,\n",
        "            'source': source\n",
        "        })\n",
        "\n",
        "        return {\n",
        "            'respuesta': response,\n",
        "            'fuente': source,\n",
        "            'intencion': intent\n",
        "        }\n",
        "\n",
        "    def get_history(self):\n",
        "        \"\"\"Retorna el historial conversacional\"\"\"\n",
        "        return self.conversation_history\n",
        "\n",
        "rag_system = RAGSystem()\n",
        "print(\"Sistema RAG creado con éxito\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6l5NI2Rgby-",
        "outputId": "fd3a5870-7db3-4f6f-d251-1276553e67cf"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sistema RAG creado con éxito\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplos de uso del Sistema RAG"
      ],
      "metadata": {
        "id": "tXejcgjP_pQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preguntas_prueba = [\n",
        "    \"¿Cómo uso mi licuadora para hacer smoothies?\",\n",
        "    \"¿Cuáles son las licuadoras de menos de $500?\",\n",
        "    \"¿Qué productos están relacionados con la categoría Cocina?\",\n",
        "]\n",
        "\n",
        "for i, pregunta in enumerate(preguntas_prueba, 1):\n",
        "    print(f\"\\n PRUEBA {i}:\")\n",
        "    print(f\"Pregunta: {pregunta}\")\n",
        "\n",
        "    resultado = rag_system.generate_response(pregunta)\n",
        "\n",
        "    print(f\"\\nIntención detectada: {resultado['intencion']}\")\n",
        "    print(f\"Fuente consultada: {resultado['fuente']}\")\n",
        "    print(f\"\\nRespuesta:\")\n",
        "    print(resultado['respuesta'])\n"
      ],
      "metadata": {
        "id": "PrghCyDzgi3-",
        "outputId": "388bdd1d-35b6-46c0-82de-673383c8dbf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " PRUEBA 1:\n",
            "Pregunta: ¿Cómo uso mi licuadora para hacer smoothies?\n",
            "\n",
            "Intención detectada: vectorial\n",
            "Fuente consultada: búsqueda semántica\n",
            "\n",
            "Respuesta:\n",
            "Basándome en la información disponible, puedo responder a tu consulta sobre: ¿Cómo uso mi licuadora para hacer smoothies?\n",
            "\n",
            "Inst... La documentación indica detalles relevantes que responden tu pregunta.\n",
            "\n",
            " PRUEBA 2:\n",
            "Pregunta: ¿Cuáles son las licuadoras de menos de $500?\n",
            "\n",
            "Intención detectada: tabular\n",
            "Fuente consultada: base de datos tabular\n",
            "\n",
            "Respuesta:\n",
            "Basándome en la información disponible, puedo responder a tu consulta sobre: ¿Cuáles son las licuadoras de menos de $500?\n",
            "\n",
            "Inst... La documentación indica detalles relevantes que responden tu pregunta.\n",
            "\n",
            " PRUEBA 3:\n",
            "Pregunta: ¿Qué productos están relacionados con la categoría Cocina?\n",
            "\n",
            "Intención detectada: grafos\n",
            "Fuente consultada: grafo de conocimiento\n",
            "\n",
            "Respuesta:\n",
            "Basándome en la información disponible, puedo responder a tu consulta sobre: ¿Qué productos están relacionados con la categoría... La documentación indica detalles relevantes que responden tu pregunta.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Buscar en la base tabular\n",
        "def table_search(filtros: dict, tabla: str = 'productos') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Busca en las tablas aplicando filtros dinámicos\n",
        "\n",
        "    Args:\n",
        "        filtros: Diccionario con filtros a aplicar\n",
        "        tabla: 'productos' o 'inventario'\n",
        "\n",
        "    Returns:\n",
        "        DataFrame filtrado\n",
        "    \"\"\"\n",
        "    if tabla == 'productos':\n",
        "        df = df_productos.copy()\n",
        "    elif tabla == 'inventario':\n",
        "        df = df_inventario.copy()\n",
        "    else:\n",
        "        raise ValueError(f\"Tabla '{tabla}' no válida\")\n",
        "\n",
        "    for columna, valor in filtros.items():\n",
        "        if columna in df.columns:\n",
        "            if isinstance(valor, dict):\n",
        "                if 'min' in valor:\n",
        "                    df = df[df[columna] >= valor['min']]\n",
        "                if 'max' in valor:\n",
        "                    df = df[df[columna] <= valor['max']]\n",
        "            elif isinstance(valor, list):\n",
        "                df = df[df[columna].isin(valor)]\n",
        "            else:\n",
        "                df = df[df[columna] == valor]\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"Función table_search() creada con éxito\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oUSwtRl4dXB",
        "outputId": "0d477219-3b0d-4c17-aa5c-0d31f4f6d0fd"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función table_search() creada con éxito\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PARTE 2**"
      ],
      "metadata": {
        "id": "iK80ZpEB7FeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creación de Herramientas**"
      ],
      "metadata": {
        "id": "fqJ0M8y7mDKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TOOL 1: doc_search_tool"
      ],
      "metadata": {
        "id": "jZu3GN2ampu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tool 1: Búsqueda en documentos\n",
        "def doc_search_tool_func(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Busca información en manuales de productos, FAQs y reseñas de usuarios.\n",
        "    Usa búsqueda semántica híbrida (vectorial + BM25 + reranking).\n",
        "\n",
        "    Args:\n",
        "        query: Pregunta sobre uso, funcionamiento o características de productos\n",
        "\n",
        "    Returns:\n",
        "        Información encontrada en los documentos\n",
        "    \"\"\"\n",
        "    try:\n",
        "        results = hybrid_search(query, top_k=3, apply_rerank=True)\n",
        "\n",
        "        if not results:\n",
        "            return \"No se encontró información relevante en los documentos.\"\n",
        "\n",
        "        response_parts = []\n",
        "        for i, node in enumerate(results, 1):\n",
        "            text_preview = node.node.text[:300].replace('\\n', ' ')\n",
        "            response_parts.append(f\"{i}. {text_preview}...\")\n",
        "\n",
        "        return \"\\n\\n\".join(response_parts)\n",
        "    except Exception as e:\n",
        "        return f\"Error al buscar en documentos: {str(e)}\"\n",
        "\n",
        "doc_search_tool = FunctionTool.from_defaults(fn=doc_search_tool_func)\n",
        "print(\"doc_search_tool creada con éxito\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXltoxZpGI5H",
        "outputId": "47323b1e-8bdf-4c59-f878-049fb6546529"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc_search_tool creada con éxito\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TOOL 2: table_search_tool"
      ],
      "metadata": {
        "id": "gEVQfd59murf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tool 2: búsqueda en tablas\n",
        "def table_search_tool_func(query: str) -> str:\n",
        "    \"\"\"Tool 2: Búsqueda tabular\"\"\"\n",
        "    try:\n",
        "        analysis = universal_query_analyzer(query)\n",
        "        sql_query = generate_sql_completely_dynamic(analysis)\n",
        "        resultado = pd.read_sql_query(sql_query, engine)\n",
        "\n",
        "        if len(resultado) == 0:\n",
        "            return \"No se encontraron productos que cumplan esos criterios.\"\n",
        "\n",
        "        productos_info = []\n",
        "        for _, row in resultado.iterrows():\n",
        "            productos_info.append(f\"• {row['nombre']} - ${row['precio_usd']:.2f} (Stock: {row['stock']})\")\n",
        "\n",
        "        return f\"Encontrados {len(resultado)} productos:\\n\" + \"\\n\".join(productos_info)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error en búsqueda tabular: {str(e)}\"\n",
        "\n",
        "def generate_sql_dynamic(query: str) -> str:\n",
        "    \"\"\"Generador SQL dinámico\"\"\"\n",
        "    import re\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    sql = \"SELECT id_producto, nombre, precio_usd, categoria, marca, stock FROM productos WHERE 1=1\"\n",
        "\n",
        "    #Precios dinámicos\n",
        "    precio_nums = re.findall(r'\\d+', query_lower)\n",
        "    if precio_nums:\n",
        "        precio = int(precio_nums[0])\n",
        "        if any(word in query_lower for word in ['menos', 'bajo', 'barato', 'económico']):\n",
        "            sql += f\" AND precio_usd < {precio}\"\n",
        "        elif any(word in query_lower for word in ['más', 'sobre', 'caro', 'premium']):\n",
        "            sql += f\" AND precio_usd > {precio}\"\n",
        "\n",
        "    #Productos dinámicos\n",
        "    if any(prod in query_lower for prod in ['licuador', 'cafetera', 'heladera', 'aire']):\n",
        "        for prod in ['licuador', 'cafetera', 'heladera', 'aire']:\n",
        "            if prod in query_lower:\n",
        "                sql += f\" AND nombre LIKE '%{prod}%'\"\n",
        "                break\n",
        "\n",
        "    #Stock dinámico\n",
        "    if any(word in query_lower for word in ['stock', 'disponible', 'buen stock']):\n",
        "        sql += \" AND stock > 50\"\n",
        "\n",
        "    sql += \" ORDER BY precio_usd LIMIT 10;\"\n",
        "    return sql\n",
        "\n"
      ],
      "metadata": {
        "id": "IX0UPoijmYZc"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def universal_query_analyzer(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    Analizador universal de consultas\n",
        "    Usa análisis inteligente para extraer información dinámicamente.\n",
        "    \"\"\"\n",
        "    schema_info = get_dynamic_schema()\n",
        "    return analyze_query_intelligently(query, schema_info)\n",
        "\n",
        "def get_dynamic_schema() -> str:\n",
        "    \"\"\"Genera descripción dinámica del esquema\"\"\"\n",
        "    productos = df_productos['nombre'].unique()[:10]\n",
        "    categorias = df_productos['categoria'].unique()\n",
        "    marcas = df_productos['marca'].unique()[:5]\n",
        "    columnas_ventas = df_ventas.columns.tolist()\n",
        "\n",
        "    return f\"\"\"\n",
        "PRODUCTOS DISPONIBLES: {productos.tolist()}\n",
        "CATEGORIAS: {categorias.tolist()}\n",
        "MARCAS: {marcas.tolist()}\n",
        "COLUMNAS PRODUCTOS: {df_productos.columns.tolist()}\n",
        "COLUMNAS VENTAS: {columnas_ventas}\n",
        "RANGO PRECIOS: ${df_productos['precio_usd'].min():.2f} - ${df_productos['precio_usd'].max():.2f}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "API_KEY = None\n",
        "\n",
        "def setup_gemini_api():\n",
        "    global API_KEY\n",
        "    if API_KEY is None:\n",
        "        API_KEY = input(\"API Key de Gemini (o Enter para usar fallback): \").strip()\n",
        "\n",
        "    if API_KEY:\n",
        "        genai.configure(api_key=API_KEY)\n",
        "        return genai.GenerativeModel('gemini-2.5-flash')\n",
        "    return None\n",
        "\n",
        "def analyze_query_intelligently(query: str, schema_info: str) -> dict:\n",
        "    \"\"\"\n",
        "    ANÁLISIS REAL CON GEMINI - Configuración que funciona\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = setup_gemini_api()\n",
        "        if not model:\n",
        "          return fallback_analysis_simple(query)\n",
        "        prompt = f\"\"\"\n",
        "SISTEMA RAG: Analiza esta consulta de usuario y genera respuesta JSON estructurada.\n",
        "\n",
        "CONSULTA DEL USUARIO: \"{query}\"\n",
        "\n",
        "DATOS DISPONIBLES:\n",
        "{schema_info}\n",
        "\n",
        "TAREA: Responde SOLO con JSON válido siguiendo esta estructura exacta:\n",
        "\n",
        "{{\n",
        "  \"tipo_consulta\": \"vectorial|tabular|grafos|analytics\",\n",
        "  \"razonamiento\": \"explicación breve de tu decisión\",\n",
        "  \"extraccion_datos\": {{\n",
        "    \"necesita_filtro_precio\": true/false,\n",
        "    \"precio_numero\": numero_o_null,\n",
        "    \"precio_operador\": \"menor_que|mayor_que|null\",\n",
        "    \"producto_mencionado\": \"nombre_producto_o_null\",\n",
        "    \"necesita_filtro_stock\": true/false,\n",
        "    \"categoria_mencionada\": \"categoria_o_null\",\n",
        "    \"busca_marcas\": true/false,\n",
        "    \"tipo_analisis\": \"ranking|distribucion|comparacion|null\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "REGLAS DE CLASIFICACIÓN:\n",
        "1. \"vectorial\": Preguntas sobre uso, funcionamiento, características\n",
        "2. \"tabular\": Búsquedas de productos con filtros\n",
        "3. \"grafos\": Relaciones, categorías, marcas\n",
        "4. \"analytics\": Análisis, estadísticas, gráficos\n",
        "\n",
        "RESPUESTA REQUERIDA: Solo JSON limpio, sin explicaciones adicionales.\n",
        "\"\"\"\n",
        "\n",
        "        # Generar respuesta con Gemini\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        # Limpiar y parsear JSON\n",
        "        try:\n",
        "            clean_response = response.text.strip()\n",
        "\n",
        "            # Remover markdown si existe\n",
        "            if clean_response.startswith('```json'):\n",
        "                clean_response = clean_response.replace('```json', '').replace('```', '').strip()\n",
        "            elif clean_response.startswith('```'):\n",
        "                clean_response = clean_response.replace('```', '').strip()\n",
        "\n",
        "            # Parsear JSON\n",
        "            analysis_result = json.loads(clean_response)\n",
        "\n",
        "            # Validar estructura\n",
        "            required_keys = [\"tipo_consulta\", \"extraccion_datos\"]\n",
        "            if not all(key in analysis_result for key in required_keys):\n",
        "                raise ValueError(\"Respuesta de Gemini no válida\")\n",
        "\n",
        "            # Log exitoso\n",
        "            print(f\"🤖 GEMINI REAL - Tipo detectado: {analysis_result['tipo_consulta']}\")\n",
        "            return analysis_result\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error parsing JSON de Gemini: {e}\")\n",
        "            print(f\"Respuesta raw: {response.text}\")\n",
        "            return fallback_analysis_simple(query)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error conectando con Gemini API: {e}\")\n",
        "        return fallback_analysis_simple(query)\n",
        "\n",
        "def fallback_analysis_simple(query: str) -> dict:\n",
        "    \"\"\"Análisis de respaldo si falla Gemini API\"\"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    # Lógica básica de respaldo\n",
        "    if any(word in query_lower for word in ['cómo', 'usar', 'funciona', 'manual']):\n",
        "        tipo = \"vectorial\"\n",
        "    elif any(word in query_lower for word in ['análisis', 'gráfico', 'top', 'estadística']):\n",
        "        tipo = \"analytics\"\n",
        "    elif any(word in query_lower for word in ['categoría', 'marca', 'relación']):\n",
        "        tipo = \"grafos\"\n",
        "    else:\n",
        "        tipo = \"tabular\"\n",
        "\n",
        "    return {\n",
        "        \"tipo_consulta\": tipo,\n",
        "        \"razonamiento\": f\"Análisis de respaldo - detectado como {tipo}\",\n",
        "        \"extraccion_datos\": {\n",
        "            \"necesita_filtro_precio\": \"precio\" in query_lower or \"$\" in query_lower,\n",
        "            \"precio_numero\": None,\n",
        "            \"precio_operador\": None,\n",
        "            \"producto_mencionado\": None,\n",
        "            \"necesita_filtro_stock\": \"stock\" in query_lower,\n",
        "            \"categoria_mencionada\": None,\n",
        "            \"busca_marcas\": \"marca\" in query_lower,\n",
        "            \"tipo_analisis\": None\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(\"🚀 GEMINI API INTEGRADA - Sistema con cerebro real activado!\")\n",
        "\n",
        "def generate_sql_completely_dynamic(analysis: dict) -> str:\n",
        "    \"\"\"Generador SQL\"\"\"\n",
        "    extraccion = analysis[\"extraccion_datos\"]\n",
        "\n",
        "    sql = \"SELECT id_producto, nombre, precio_usd, categoria, marca, stock FROM productos WHERE 1=1\"\n",
        "\n",
        "    if extraccion[\"necesita_filtro_precio\"] and extraccion[\"precio_numero\"]:\n",
        "        precio = extraccion[\"precio_numero\"]\n",
        "        if extraccion[\"precio_operador\"] == \"menor_que\":\n",
        "            sql += f\" AND precio_usd < {precio}\"\n",
        "        elif extraccion[\"precio_operador\"] == \"mayor_que\":\n",
        "            sql += f\" AND precio_usd > {precio}\"\n",
        "\n",
        "    if extraccion[\"producto_mencionado\"]:\n",
        "        producto = extraccion[\"producto_mencionado\"]\n",
        "        sql += f\" AND nombre LIKE '%{producto}%'\"\n",
        "\n",
        "    if extraccion[\"necesita_filtro_stock\"]:\n",
        "        sql += \" AND stock > 50\"\n",
        "\n",
        "    sql += \" ORDER BY precio_usd LIMIT 10;\"\n",
        "    return sql\n",
        "\n",
        "def generate_analytics_sql_dynamic(analysis: dict) -> str:\n",
        "    \"\"\"Generador analytics SQL dinámico\"\"\"\n",
        "    tipo_analisis = analysis[\"extraccion_datos\"][\"tipo_analisis\"]\n",
        "\n",
        "    if tipo_analisis == \"distribución\":\n",
        "        return \"SELECT metodo_pago, COUNT(*) as cantidad FROM ventas GROUP BY metodo_pago ORDER BY cantidad DESC\"\n",
        "    elif tipo_analisis == \"ranking\":\n",
        "        return \"SELECT v.nombre_producto, SUM(v.cantidad) as total_vendido FROM ventas v GROUP BY v.nombre_producto ORDER BY total_vendido DESC LIMIT 10\"\n",
        "    else:\n",
        "        return \"SELECT cliente_provincia, COUNT(*) as ventas FROM ventas GROUP BY cliente_provincia ORDER BY ventas DESC\"\n",
        "table_search_tool = FunctionTool.from_defaults(fn=table_search_tool_func)"
      ],
      "metadata": {
        "id": "DGg8E7lnQ-CJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed794b4-0c84-473b-880b-b2718d8a8dde"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 GEMINI API INTEGRADA - Sistema con cerebro real activado!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TOOL 3: graph_search_tool\n"
      ],
      "metadata": {
        "id": "Cv_XqF5Kmzzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tool 3: búsqueda en grafos\n",
        "def graph_search_tool_func(query: str) -> str:\n",
        "    \"\"\"Búsqueda en grafos\"\"\"\n",
        "\n",
        "    try:\n",
        "        analysis = universal_query_analyzer(query)\n",
        "        cypher_query = generate_cypher_dynamic(analysis)\n",
        "\n",
        "        print(f\"🔗 Cypher: {cypher_query}\")\n",
        "\n",
        "        if cypher_query.startswith(\"SQL_MARCAS\"):\n",
        "            return handle_sql_marcas_fallback(cypher_query, analysis)\n",
        "\n",
        "        with driver.session() as session:\n",
        "            result = session.run(cypher_query)\n",
        "            records = list(result)\n",
        "\n",
        "        if not records:\n",
        "            return \"No se encontraron relaciones para esa consulta.\"\n",
        "\n",
        "        #Convierte IDs a nombres reales\n",
        "        items = []\n",
        "        for record in records:\n",
        "            value = list(record.values())[0]\n",
        "\n",
        "            if isinstance(value, str) and value.startswith('P'):\n",
        "                try:\n",
        "                    nombre_real = df_productos[df_productos['id_producto'] == value]['nombre'].iloc[0]\n",
        "                    items.append(f\"• {nombre_real}\")\n",
        "                except:\n",
        "                    items.append(f\"• {value}\")\n",
        "            else:\n",
        "                items.append(f\"• {value}\")\n",
        "\n",
        "        return f\"Encontradas {len(records)} relaciones:\\n\" + \"\\n\".join(items[:10])\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error en búsqueda de grafos: {str(e)}\"\n",
        "\n",
        "def generate_cypher_dynamic(analysis: dict) -> str:\n",
        "    \"\"\"Generador Cypher dinámico mejorado - VERSIÓN FINAL IMPECABLE\"\"\"\n",
        "\n",
        "    extraccion = analysis[\"extraccion_datos\"]\n",
        "\n",
        "    #Detectar si busca marcas específicamente\n",
        "    if extraccion.get(\"busca_marcas\", False):\n",
        "        if extraccion.get(\"categoria_mencionada\"):\n",
        "            categoria = extraccion[\"categoria_mencionada\"].title()\n",
        "            return f\"SQL_MARCAS_CATEGORIA_{categoria}\"\n",
        "        else:\n",
        "            return \"SQL_MARCAS_TODAS\"\n",
        "\n",
        "    #Productos por categoría\n",
        "    elif extraccion.get(\"categoria_mencionada\"):\n",
        "        categoria = extraccion[\"categoria_mencionada\"].title()\n",
        "        return f\"\"\"\n",
        "        MATCH (p:Producto)-[:PERTENECE_A]->(c:Categoria)\n",
        "        WHERE c.name CONTAINS '{categoria}'\n",
        "        RETURN p.name as producto\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "    #Queries genéricas\n",
        "    elif \"sucursal\" in analysis.get(\"razonamiento\", \"\").lower():\n",
        "        return \"\"\"\n",
        "        MATCH (s:Sucursal)\n",
        "        RETURN s.name as sucursal\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "    elif \"vendedor\" in analysis.get(\"razonamiento\", \"\").lower():\n",
        "        return \"\"\"\n",
        "        MATCH (v:Vendedor)\n",
        "        RETURN v.name as vendedor\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "    else:\n",
        "        #Query por defecto\n",
        "        return \"\"\"\n",
        "        MATCH (c:Categoria)\n",
        "        RETURN c.name as categoria\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "def handle_sql_marcas_fallback(sql_signal: str, analysis: dict) -> str:\n",
        "    \"\"\"Maneja consultas de marcas usando SQL en lugar de Neo4j\"\"\"\n",
        "\n",
        "    try:\n",
        "        if \"CATEGORIA\" in sql_signal:\n",
        "            # Extraer categoría del signal\n",
        "            categoria = analysis[\"extraccion_datos\"][\"categoria_mencionada\"]\n",
        "            marcas = df_productos[df_productos['categoria'].str.lower() == categoria.lower()]['marca'].unique()\n",
        "\n",
        "            if len(marcas) == 0:\n",
        "                return f\"No se encontraron marcas en la categoría {categoria}.\"\n",
        "\n",
        "            items = [f\"• {marca}\" for marca in marcas[:10]]\n",
        "            return f\"Encontradas {len(marcas)} marcas en {categoria}:\\n\" + \"\\n\".join(items)\n",
        "\n",
        "        else:  # SQL_MARCAS_TODAS\n",
        "            marcas = df_productos['marca'].unique()\n",
        "            items = [f\"• {marca}\" for marca in marcas[:10]]\n",
        "            return f\"Encontradas {len(marcas)} marcas:\\n\" + \"\\n\".join(items)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error en consulta de marcas: {str(e)}\"\n",
        "\n",
        "graph_search_tool = FunctionTool.from_defaults(fn=graph_search_tool_func)"
      ],
      "metadata": {
        "id": "dwP6ObaOmQCQ"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TOOL 4: analytics_tool_func (SQL + Gráficos)"
      ],
      "metadata": {
        "id": "miZvyAMim5Mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engine = create_engine('sqlite:///ventas_analytics.db')\n",
        "\n",
        "#Guardar DataFrames en SQL\n",
        "df_productos.to_sql('productos', engine, if_exists='replace', index=False)\n",
        "df_ventas.to_sql('ventas', engine, if_exists='replace', index=False)\n",
        "df_inventario.to_sql('inventario', engine, if_exists='replace', index=False)\n",
        "\n",
        "print(\"Base de datos SQL creada con 3 tablas\")\n",
        "\n",
        "#Configurar Langchain SQL Database\n",
        "db = SQLDatabase(engine=engine)\n",
        "print(f\"   Tablas disponibles: {db.get_usable_table_names()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieg2OPeBGi-y",
        "outputId": "f3393469-0987-4f3b-c124-a4f5b50b44a1"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base de datos SQL creada con 3 tablas\n",
            "   Tablas disponibles: ['inventario', 'productos', 'ventas']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TOOL 4: gráficos\n",
        "\n",
        "def analytics_tool_func(query: str) -> str:\n",
        "    \"\"\"Tool 4: Analytics\"\"\"\n",
        "    try:\n",
        "        analysis = universal_query_analyzer(query)\n",
        "        sql_query = generate_analytics_sql_dynamic(analysis)\n",
        "        resultado = pd.read_sql_query(sql_query, engine)\n",
        "\n",
        "        if len(resultado) == 0:\n",
        "            return \"No hay datos disponibles para esta consulta analítica.\"\n",
        "\n",
        "        # Generar gráfico dinámicamente\n",
        "        if analysis[\"extraccion_datos\"][\"tipo_analisis\"]:\n",
        "            filename = generate_dynamic_chart(resultado, query)\n",
        "            return f\"Análisis completado. Datos procesados: {len(resultado)} registros.\\nGráfico generado: {filename}\"\n",
        "        else:\n",
        "            datos = \"\\n\".join([f\"• {row.iloc[0]}: {row.iloc[1]}\" for _, row in resultado.iterrows()])\n",
        "            return f\"Resultados del análisis ({len(resultado)} registros):\\n{datos}\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error en análisis: {str(e)}\"\n",
        "\n",
        "def generate_analytics_sql(query: str) -> str:\n",
        "    \"\"\"Generador SQL dinámico para análisis\"\"\"\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    if any(palabra in query_lower for palabra in ['método', 'metodo', 'pago', 'pagos']):\n",
        "        return \"\"\"\n",
        "        SELECT metodo_pago, COUNT(*) as cantidad\n",
        "        FROM ventas\n",
        "        GROUP BY metodo_pago\n",
        "        ORDER BY cantidad DESC\n",
        "        \"\"\"\n",
        "\n",
        "    elif any(palabra in query_lower for palabra in ['vendido', 'vendidos', 'popular', 'populares', 'top']):\n",
        "        return \"\"\"\n",
        "        SELECT v.nombre_producto, SUM(v.cantidad) as total_vendido\n",
        "        FROM ventas v\n",
        "        GROUP BY v.nombre_producto\n",
        "        ORDER BY total_vendido DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "    elif any(palabra in query_lower for palabra in ['provincia', 'provincias', 'región', 'region']):\n",
        "        return \"\"\"\n",
        "        SELECT cliente_provincia, COUNT(*) as ventas\n",
        "        FROM ventas\n",
        "        GROUP BY cliente_provincia\n",
        "        ORDER BY ventas DESC\n",
        "        \"\"\"\n",
        "\n",
        "    elif any(palabra in query_lower for palabra in ['sucursal', 'sucursales', 'tienda', 'tiendas']):\n",
        "        return \"\"\"\n",
        "        SELECT sucursal, COUNT(*) as ventas\n",
        "        FROM ventas\n",
        "        GROUP BY sucursal\n",
        "        ORDER BY ventas DESC\n",
        "        \"\"\"\n",
        "\n",
        "    else:\n",
        "        return \"\"\"\n",
        "        SELECT 'Total Ventas' as concepto, COUNT(*) as cantidad\n",
        "        FROM ventas\n",
        "        \"\"\"\n",
        "\n",
        "def generate_dynamic_chart(data: pd.DataFrame, query: str) -> str:\n",
        "    \"\"\"Generador dinámico de gráficos\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import time\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    #Detecta tipo de gráfico según consulta\n",
        "    query_lower = query.lower()\n",
        "\n",
        "    if 'distribución' in query_lower or 'distribucion' in query_lower:\n",
        "        # Gráfico de torta\n",
        "        plt.pie(data.iloc[:, 1], labels=data.iloc[:, 0], autopct='%1.1f%%')\n",
        "        plt.title(f\"Distribución: {data.columns[0]}\")\n",
        "    else:\n",
        "        #Gráfico de barras\n",
        "        plt.bar(data.iloc[:, 0], data.iloc[:, 1])\n",
        "        plt.title(f\"Análisis: {data.columns[0]} vs {data.columns[1]}\")\n",
        "        plt.xlabel(data.columns[0])\n",
        "        plt.ylabel(data.columns[1])\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    timestamp = int(time.time())\n",
        "    filename = f'analytics_dynamic_{timestamp}.png'\n",
        "    filepath = f'/tmp/{filename}'\n",
        "\n",
        "    plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filename\n",
        "analytics_tool = FunctionTool.from_defaults(fn=analytics_tool_func)"
      ],
      "metadata": {
        "id": "nLlJ7sGBExAb"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generar los 4 gráficos de ejemplo\n",
        "queries_test = [\n",
        "    \"Dame un gráfico sobre la distribución de los métodos de pago\",\n",
        "    \"Muéstrame los productos más vendidos\",\n",
        "    \"Análisis de ventas por provincia\",\n",
        "    \"Ventas por sucursal\"\n",
        "]\n",
        "\n",
        "print(\"GENERANDO LOS GRÁFICOS\")\n",
        "\n",
        "\n",
        "for i, query in enumerate(queries_test, 1):\n",
        "    print(f\"\\n{i}. Ejecutando: {query}\")\n",
        "    resultado = analytics_tool.fn(query)\n",
        "    print(f\"    Completado\")\n",
        "\n",
        "print(\"\\n Gráficos generados con éxito\")\n",
        "print(\"\\nArchivos en /tmp/:\")\n",
        "import os\n",
        "archivos = [f for f in os.listdir('/tmp/') if f.startswith('analytics_')]\n",
        "for archivo in archivos:\n",
        "    print(f\"  - {archivo}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "miy5h3uKE4lv",
        "outputId": "ee0d4d77-ba7d-4078-bb03-3ce00855b811"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERANDO LOS GRÁFICOS\n",
            "\n",
            "1. Ejecutando: Dame un gráfico sobre la distribución de los métodos de pago\n",
            "API Key de Gemini (o Enter para usar fallback): AIzaSyAxBTwhtl2fWihZBUUdGvDdFHeysTi13FE\n",
            "🤖 GEMINI REAL - Tipo detectado: analytics\n",
            "    Completado\n",
            "\n",
            "2. Ejecutando: Muéstrame los productos más vendidos\n",
            "🤖 GEMINI REAL - Tipo detectado: analytics\n",
            "    Completado\n",
            "\n",
            "3. Ejecutando: Análisis de ventas por provincia\n",
            "🤖 GEMINI REAL - Tipo detectado: analytics\n",
            "    Completado\n",
            "\n",
            "4. Ejecutando: Ventas por sucursal\n",
            "🤖 GEMINI REAL - Tipo detectado: analytics\n",
            "    Completado\n",
            "\n",
            " Gráficos generados con éxito\n",
            "\n",
            "Archivos en /tmp/:\n",
            "  - analytics_dynamic_1766728482.png\n",
            "  - analytics_dynamic_1766725611.png\n",
            "  - analytics_dynamic_1766728957.png\n",
            "  - analytics_dynamic_1766728493.png\n",
            "  - analytics_dynamic_1766725612.png\n",
            "  - analytics_dynamic_1766728964.png\n",
            "  - analytics_dynamic_1766728488.png\n",
            "  - analytics_dynamic_1766728961.png\n",
            "  - analytics_dynamic_1766725613.png\n",
            "  - analytics_dynamic_1766728967.png\n",
            "  - analytics_dynamic_1766728485.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creación del agente React, definición de System Prompt y memoria para contexto**"
      ],
      "metadata": {
        "id": "FdfyiPyVkvtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defino el system Prompt\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Eres un asistente experto en electrodomésticos que ayuda a los usuarios con información sobre productos.\n",
        "\n",
        "**HERRAMIENTAS DISPONIBLES:**\n",
        "- doc_search_tool: Busca en manuales, FAQs y documentación de productos\n",
        "- table_search_tool: Busca productos por precio, stock, categoría, marca\n",
        "- graph_search_tool: Busca relaciones entre productos, categorías y sucursales\n",
        "- analytics_tool: Genera análisis y estadísticas de ventas\n",
        "\n",
        "**REGLAS OBLIGATORIAS:**\n",
        "1. SIEMPRE sigue el proceso: Thought -> Action -> Observation -> Final Answer\n",
        "2. USA las herramientas apropiadas según la pregunta\n",
        "3. Si la pregunta tiene múltiples partes, usa las herramientas una por una\n",
        "4. NO inventes información, siempre usa las herramientas\n",
        "5. Responde en español de forma clara y útil\n",
        "\n",
        "**EJEMPLOS:**\n",
        "\n",
        "Pregunta: ¿Cómo uso mi licuadora?\n",
        "Thought: Esta pregunta es sobre uso de producto, debo buscar en documentación.\n",
        "Action: doc_search_tool\n",
        "Action Input: {\"query\": \"cómo usar licuadora\"}\n",
        "Observation: [Información del manual]\n",
        "Thought: Ya tengo la información necesaria.\n",
        "Final Answer: Según el manual, para usar la licuadora...\n",
        "\n",
        "Pregunta: ¿Cuáles son las licuadoras de menos de $300?\n",
        "Thought: Esta pregunta requiere filtrar productos por precio.\n",
        "Action: table_search_tool\n",
        "Action Input: {\"query\": \"licuadoras menos de 300\"}\n",
        "Observation: [Lista de productos]\n",
        "Final Answer: Estas son las licuadoras disponibles por menos de $300...\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n System Prompt definido\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJJCyHHHGzfS",
        "outputId": "2ab48771-5782-4c09-d6d4-351733c87d6c"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " System Prompt definido\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleReActAgent:\n",
        "    \"\"\"Agente ReAct con manejo de respuestas vacías\"\"\"\n",
        "\n",
        "    def __init__(self, tools_list):\n",
        "        self.tools = {\n",
        "            'doc_search': tools_list[0],\n",
        "            'table_search': tools_list[1],\n",
        "            'graph_search': tools_list[2],\n",
        "            'analytics': tools_list[3]\n",
        "        }\n",
        "\n",
        "    def _is_empty_response(self, observation: str) -> bool:\n",
        "        \"\"\"Detecta si la respuesta está vacía o no tiene información útil\"\"\"\n",
        "        if not observation or len(observation.strip()) < 10:\n",
        "            return True\n",
        "\n",
        "        #Frases que indican respuesta vacía\n",
        "        empty_indicators = [\n",
        "            'no se encontr',\n",
        "            'no hay',\n",
        "            'no existe',\n",
        "            'sin resultados',\n",
        "            'ningún',\n",
        "            'ningun',\n",
        "            'error'\n",
        "        ]\n",
        "\n",
        "        observation_lower = observation.lower()\n",
        "        return any(indicator in observation_lower for indicator in empty_indicators)\n",
        "\n",
        "    def chat(self, query: str) -> str:\n",
        "        \"\"\"Ejecuta el ciclo ReAct: Thought -> Action -> Observation -> Answer\"\"\"\n",
        "\n",
        "        query_lower = query.lower()\n",
        "\n",
        "        palabras_analytics = ['análisis', 'analisis', 'top', 'más vendido', 'mas vendido',\n",
        "                             'estadística', 'estadistica', 'gráfico', 'grafico', 'comparar']\n",
        "\n",
        "        if any(palabra in query_lower for palabra in palabras_analytics):\n",
        "            tool_name = 'analytics'\n",
        "            tool = self.tools['analytics']\n",
        "            intent = 'analytics'\n",
        "        else:\n",
        "            #Clasificar intención\n",
        "            intent = classify_intent(query)\n",
        "\n",
        "            if intent == 'vectorial':\n",
        "                tool_name = 'doc_search'\n",
        "                tool = self.tools['doc_search']\n",
        "            elif intent == 'tabular':\n",
        "                tool_name = 'table_search'\n",
        "                tool = self.tools['table_search']\n",
        "            elif intent == 'grafos':\n",
        "                tool_name = 'graph_search'\n",
        "                tool = self.tools['graph_search']\n",
        "            else:\n",
        "                tool_name = 'doc_search'\n",
        "                tool = self.tools['doc_search']\n",
        "\n",
        "        observation = tool.fn(query)\n",
        "\n",
        "        #Verifica si la respuesta está vacía\n",
        "        if self._is_empty_response(observation):\n",
        "            final_answer = f\"\"\"No pude encontrar información relevante para tu consulta.\n",
        "\n",
        "Sugerencias para reformular tu pregunta:\n",
        "- Intenta ser más específico sobre lo que buscas\n",
        "- Usa términos diferentes o sinónimos\n",
        "- Si buscas un producto, menciona características concretas (precio, categoría, marca)\n",
        "- Si buscas información de uso, especifica el producto\n",
        "\n",
        "Ejemplos de consultas efectivas:\n",
        "- \"¿Cuáles son las licuadoras de menos de $300?\"\n",
        "- \"¿Cómo usar mi licuadora para hacer smoothies?\"\n",
        "- \"¿Qué marcas tienen productos de climatización?\"\n",
        "\n",
        "Observación técnica: {observation}\"\"\"\n",
        "        else:\n",
        "            final_answer = observation[:500] + (\"...\" if len(observation) > 500 else \"\")\n",
        "\n",
        "        #Construir respuesta ReAct\n",
        "        response = f\"\"\"\n",
        "=== Proceso ReAct ===\n",
        "\n",
        "Thought: Analizo la consulta \"{query}\".\n",
        "Detecto que requiere información de tipo '{intent}'.\n",
        "\n",
        "Action: Voy a usar la herramienta '{tool_name}' para obtener la información necesaria.\n",
        "\n",
        "Observation:\n",
        "{observation}\n",
        "\n",
        "Final Answer:\n",
        "{final_answer}\n",
        "\n",
        "(Basado en la información encontrada usando {tool_name})\n",
        "\"\"\"\n",
        "        return response\n",
        "\n",
        "#Crear agente inicial\n",
        "analytics_tool = FunctionTool.from_defaults(fn=analytics_tool_func)\n",
        "tools = [doc_search_tool, table_search_tool, graph_search_tool, analytics_tool]\n",
        "agent = SimpleReActAgent(tools)\n",
        "\n",
        "print(\"Agente ReAct creado\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dSbPIDMG1pX",
        "outputId": "5ec7f432-ea97-40b4-f1cc-543accce4fe5"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agente ReAct creado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Memoria para contexto"
      ],
      "metadata": {
        "id": "rSa-CC3-OlSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConversationalAgent:\n",
        "    \"\"\"Agente ReAct con memoria conversacional\"\"\"\n",
        "\n",
        "    def __init__(self, base_agent):\n",
        "        self.agent = base_agent\n",
        "        self.history = []\n",
        "        self.max_history = 5\n",
        "        self.last_topic = None\n",
        "\n",
        "    def _extract_topic(self, query: str) -> str:\n",
        "        \"\"\"Extrae el tema principal (sustantivos clave)\"\"\"\n",
        "        query_clean = query.lower()\n",
        "        query_clean = query_clean.replace('¿', '').replace('?', '')\n",
        "\n",
        "        topics = ['licuadora', 'licuadoras', 'cafetera', 'cafeteras', 'heladera',\n",
        "                 'heladeras', 'producto', 'productos', 'aire acondicionado',\n",
        "                 'lavarropas', 'microondas', 'batidora', 'tostadora']\n",
        "\n",
        "        for topic in topics:\n",
        "            if topic in query_clean:\n",
        "                return topic\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _build_natural_query(self, topic: str, continuation: str) -> str:\n",
        "        \"\"\"\n",
        "        Construye una query natural combinando tema + continuación\n",
        "\n",
        "        Ejemplos:\n",
        "        - topic=\"licuadora\" + continuation=\"cuáles tienen buen stock?\"\n",
        "          → \"licuadoras con buen stock\"\n",
        "        - topic=\"cafetera\" + continuation=\"hay económicas?\"\n",
        "          → \"cafeteras económicas\"\n",
        "        \"\"\"\n",
        "        continuation_clean = continuation.lower().strip()\n",
        "\n",
        "        topic_plural = topic\n",
        "        if not topic.endswith('s'):\n",
        "            topic_plural = topic + 's'\n",
        "\n",
        "        if 'buen stock' in continuation_clean or 'mucho stock' in continuation_clean:\n",
        "            return f\"{topic_plural} con buen stock\"\n",
        "\n",
        "        elif 'económica' in continuation_clean or 'barata' in continuation_clean:\n",
        "            return f\"{topic_plural} económicas\"\n",
        "\n",
        "        elif 'cara' in continuation_clean or 'premium' in continuation_clean:\n",
        "            return f\"{topic_plural} premium\"\n",
        "\n",
        "        elif 'mejor' in continuation_clean or 'mejores' in continuation_clean:\n",
        "            return f\"mejores {topic_plural}\"\n",
        "\n",
        "        else:\n",
        "            continuation_clean = continuation_clean.replace('cuáles', '').replace('cuales', '')\n",
        "            continuation_clean = continuation_clean.replace('qué', '').replace('que', '')\n",
        "            continuation_clean = continuation_clean.strip()\n",
        "\n",
        "            return f\"{topic_plural} {continuation_clean}\"\n",
        "\n",
        "    def _enrich_query_with_context(self, query: str) -> str:\n",
        "        \"\"\"Enriquece la query con contexto del historial\"\"\"\n",
        "        if not self.history or not self.last_topic:\n",
        "            return query\n",
        "\n",
        "        query_clean = query.lower().strip()\n",
        "\n",
        "        continuity_indicators = ['¿y ', '¿también', 'y ', 'también', 'además', 'otro', 'otra', 'más', '¿cuáles']\n",
        "\n",
        "        has_continuity = any(query_clean.startswith(ind.lower()) for ind in continuity_indicators)\n",
        "\n",
        "        if has_continuity:\n",
        "            query_without_prefix = query\n",
        "            for ind in continuity_indicators:\n",
        "                if query_clean.startswith(ind.lower()):\n",
        "                    query_without_prefix = query[len(ind):].strip()\n",
        "                    break\n",
        "\n",
        "            enriched = self._build_natural_query(self.last_topic, query_without_prefix)\n",
        "\n",
        "            print(f\"\\n[Memoria activa] Detectada continuidad\")\n",
        "            print(f\"  Tema anterior: {self.last_topic}\")\n",
        "            print(f\"  Query original: '{query}'\")\n",
        "            print(f\"  Query enriquecida: '{enriched}'\")\n",
        "\n",
        "            return enriched\n",
        "\n",
        "        return query\n",
        "\n",
        "    def chat(self, query: str, use_history: bool = True) -> str:\n",
        "        \"\"\"Ejecuta consulta usando contexto del historial\"\"\"\n",
        "\n",
        "        current_topic = self._extract_topic(query)\n",
        "\n",
        "        if use_history:\n",
        "            enriched_query = self._enrich_query_with_context(query)\n",
        "        else:\n",
        "            enriched_query = query\n",
        "\n",
        "        response = self.agent.chat(enriched_query)\n",
        "\n",
        "        self.history.append((query, response))\n",
        "\n",
        "        if current_topic:\n",
        "            self.last_topic = current_topic\n",
        "\n",
        "        if len(self.history) > self.max_history:\n",
        "            self.history = self.history[-self.max_history:]\n",
        "\n",
        "        return response\n",
        "\n",
        "    def get_history(self) -> list:\n",
        "        return self.history.copy()\n",
        "\n",
        "    def clear_history(self):\n",
        "        self.history = []\n",
        "        self.last_topic = None\n",
        "        return \"Historial limpiado\"\n",
        "\n",
        "    def show_history(self):\n",
        "        if not self.history:\n",
        "            return \"No hay historial\"\n",
        "\n",
        "        result = \"HISTORIAL:\\n\" + \"=\"*70 + \"\\n\\n\"\n",
        "        for i, (q, r) in enumerate(self.history, 1):\n",
        "            result += f\"{i}. Usuario: {q}\\n   Asistente: {r[:150]}...\\n\\n\"\n",
        "        return result\n",
        "\n",
        "conversational_agent = ConversationalAgent(agent)\n"
      ],
      "metadata": {
        "id": "97o2xh25On_E"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generación con LLM"
      ],
      "metadata": {
        "id": "Gu2K0w40O77b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMGenerator:\n",
        "    \"\"\"\n",
        "    Sistema de generación de respuestas con LLM.\n",
        "\n",
        "    NOTA IMPORTANTE:\n",
        "    Por limitaciones de API key, esta versión usa un generador simplificado.\n",
        "    En producción, este componente usaría Google Gemini Flash 1.5.\n",
        "\n",
        "    JUSTIFICACIÓN DE ELECCIÓN DE LLM (para producción):\n",
        "    - Modelo: Google Gemini Flash 1.5\n",
        "    - Alojamiento: Nube (Google Cloud)\n",
        "    - Razones:\n",
        "      1. Gratuito hasta 1500 requests/día\n",
        "      2. Multilingüe (excelente en español)\n",
        "      3. Contexto de 1M tokens\n",
        "      4. Latencia baja (~500ms)\n",
        "      5. Soporta tool calling nativo\n",
        "\n",
        "    Alternativas consideradas:\n",
        "    - GPT-4: Más caro, mejor calidad\n",
        "    - Claude: Excelente razonamiento, requiere pago\n",
        "    - Llama 3 local: Sin costo API, requiere GPU\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"gemini-1.5-flash\"):\n",
        "        self.model_name = model_name\n",
        "        self.is_mock = True\n",
        "\n",
        "    def generate(self, prompt: str, system_prompt: str = None) -> str:\n",
        "        \"\"\"\n",
        "        Genera una respuesta usando el LLM\n",
        "\n",
        "        En producción esto ejecutaría llamadas reales a la API de Gemini.\n",
        "        Ejemplo de implementación real (requiere API key):\n",
        "\n",
        "            from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "            llm = ChatGoogleGenerativeAI(\n",
        "                model=\"gemini-1.5-flash\",\n",
        "                temperature=0.7,\n",
        "                google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                SystemMessage(content=system_prompt),\n",
        "                HumanMessage(content=prompt)\n",
        "            ]\n",
        "\n",
        "            response = llm.invoke(messages)\n",
        "            return response.content\n",
        "        \"\"\"\n",
        "\n",
        "        return f\"[RESPUESTA SIMULADA - En producción usaría {self.model_name}]\\n{prompt[:200]}...\"\n",
        "\n",
        "    def get_info(self):\n",
        "        \"\"\"Retorna información sobre la configuración del LLM\"\"\"\n",
        "llm_generator = LLMGenerator()\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z66eV76dOxEh",
        "outputId": "c37f80c9-05e6-43d6-cf51-bd5e49ffce6a"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creación del clasificador y pruebas finales**"
      ],
      "metadata": {
        "id": "LakL-H_bj1vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMFewShotClassifier:\n",
        "    \"\"\"Clasificador que usa Few-Shot Learning con embeddings\"\"\"\n",
        "    def __init__(self, embed_model):\n",
        "        self.embed_model = embed_model\n",
        "        self.examples = {\n",
        "            'vectorial': [\n",
        "                \"¿Cómo uso mi licuadora?\",\n",
        "                \"¿Qué dice el manual sobre mantenimiento?\",\n",
        "                \"¿Qué opinan los usuarios sobre este producto?\"\n",
        "            ],\n",
        "            'tabular': [\n",
        "                \"¿Cuáles son las licuadoras de menos de $200?\",\n",
        "                \"¿Cuánto stock hay disponible?\",\n",
        "                \"Quiero productos de la marca ChefMaster\"\n",
        "            ],\n",
        "            'grafos': [\n",
        "                \"¿Qué productos están en la categoría Cocina?\",\n",
        "                \"¿Qué vendedores venden productos TechHome?\",\n",
        "                \"¿En qué sucursales se vende este producto?\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        #Calcular embeddings de ejemplos\n",
        "        self.example_embeddings = {}\n",
        "        for intent, examples in self.examples.items():\n",
        "            embeddings = [embed_model.get_text_embedding(ex) for ex in examples]\n",
        "            self.example_embeddings[intent] = np.array(embeddings)\n",
        "\n",
        "    def predict(self, query: str) -> str:\n",
        "        \"\"\"Clasifica la query por similitud con ejemplos\"\"\"\n",
        "        query_embedding = self.embed_model.get_text_embedding(query)\n",
        "        query_embedding = np.array(query_embedding)\n",
        "\n",
        "        #Calcular similitud con cada intención\n",
        "        similarities = {}\n",
        "        for intent, examples_emb in self.example_embeddings.items():\n",
        "            sims = np.dot(examples_emb, query_embedding) / (\n",
        "                np.linalg.norm(examples_emb, axis=1) * np.linalg.norm(query_embedding)\n",
        "            )\n",
        "            similarities[intent] = np.mean(sims)\n",
        "\n",
        "        return max(similarities, key=similarities.get)\n",
        "\n",
        "clasificador = LLMFewShotClassifier(Settings.embed_model)\n",
        "\n",
        "def classify_intent(query: str) -> str:\n",
        "    \"\"\"Clasifica la intención de una consulta del usuario\"\"\"\n",
        "    return clasificador.predict(query)\n",
        "\n",
        "print(\"\\n Clasificador creado con éxito\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4vyZqdzaJbF",
        "outputId": "f7263a1f-5b33-4cda-d786-9f8c759aa83d"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Clasificador creado con éxito\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consultas de prueba"
      ],
      "metadata": {
        "id": "wzXJywYsjomi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "consultas_prueba = [\n",
        "    \"¿Cómo uso mi licuadora para hacer smoothies?\",\n",
        "    \"¿Cuáles son las licuadoras de menos de $500?\",\n",
        "    \"¿Qué productos están en la categoría Cocina?\",\n",
        "    \"Muéstrame un análisis de los productos más vendidos\",\n",
        "    \"¿Qué marcas tienen productos de climatización?\",\n",
        "    \"Necesito una cafetera económica con buen stock\",\n",
        "]\n",
        "\n",
        "resultados_agente = []\n",
        "\n",
        "for i, consulta in enumerate(consultas_prueba, 1):\n",
        "    print(\"=\"*70)\n",
        "    print(f\"PRUEBA {i}/{len(consultas_prueba)}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Consulta: {consulta}\\n\")\n",
        "\n",
        "    try:\n",
        "        respuesta = agent.chat(consulta)\n",
        "        print(respuesta)\n",
        "        resultados_agente.append({\n",
        "            'consulta': consulta,\n",
        "            'respuesta': respuesta,\n",
        "            'exito': True\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        resultados_agente.append({\n",
        "            'consulta': consulta,\n",
        "            'respuesta': str(e),\n",
        "            'exito': False\n",
        "        })\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "print(\"RESUMEN DE PRUEBAS:\")\n",
        "exitosas = sum(1 for r in resultados_agente if r['exito'])\n",
        "print(f\"\\nPruebas exitosas: {exitosas}/{len(consultas_prueba)}\")\n",
        "\n",
        "if exitosas == len(consultas_prueba):\n",
        "    print(\"Todas las pruebas completadas exitosamente\")\n",
        "else:\n",
        "    print(f\"{len(consultas_prueba) - exitosas} pruebas fallaron\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1rbuGqM9HDYS",
        "outputId": "7fdff599-0465-475e-f1dd-cd08aae102e6"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "PRUEBA 1/6\n",
            "======================================================================\n",
            "Consulta: ¿Cómo uso mi licuadora para hacer smoothies?\n",
            "\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"¿Cómo uso mi licuadora para hacer smoothies?\".\n",
            "Detecto que requiere información de tipo 'vectorial'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'doc_search' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "1. Pregunta: ¿Cómo se usa correctamente este producto? Respuesta: El Licuadora de TechHome está diseñado para uso doméstico. Revise el manual del producto (código P0001) para más detalles. Ante cualquier duda, contacte a nuestro servicio de atención al cliente....\n",
            "\n",
            "2. Pregunta: ¿Cómo se usa correctamente este producto? Respuesta: El Licuadora de TechHome está diseñado para uso doméstico. Revise el manual del producto (código P0001) para más detalles. Ante cualquier duda, contacte a nuestro servicio de atención al cliente....\n",
            "\n",
            "3. Pregunta: ¿Cómo se usa correctamente este producto? Respuesta: El Licuadora de ChefMaster está diseñado para uso doméstico. Revise el manual del producto (código P0006) para más detalles. Ante cualquier duda, contacte a nuestro servicio de atención al cliente....\n",
            "\n",
            "Final Answer:\n",
            "1. Pregunta: ¿Cómo se usa correctamente este producto? Respuesta: El Licuadora de TechHome está diseñado para uso doméstico. Revise el manual del producto (código P0001) para más detalles. Ante cualquier duda, contacte a nuestro servicio de atención al cliente....\n",
            "\n",
            "2. Pregunta: ¿Cómo se usa correctamente este producto? Respuesta: El Licuadora de TechHome está diseñado para uso doméstico. Revise el manual del producto (código P0001) para más detalles. Ante cualquier duda, contacte a nuestro servi...\n",
            "\n",
            "(Basado en la información encontrada usando doc_search)\n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "PRUEBA 2/6\n",
            "======================================================================\n",
            "Consulta: ¿Cuáles son las licuadoras de menos de $500?\n",
            "\n",
            "🤖 GEMINI REAL - Tipo detectado: tabular\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"¿Cuáles son las licuadoras de menos de $500?\".\n",
            "Detecto que requiere información de tipo 'tabular'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'table_search' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Encontrados 3 productos:\n",
            "• Compacto Licuadora - $259.42 (Stock: 75)\n",
            "• Licuadora - $283.63 (Stock: 108)\n",
            "• Plus Licuadora Pro - $329.07 (Stock: 97)\n",
            "\n",
            "Final Answer:\n",
            "Encontrados 3 productos:\n",
            "• Compacto Licuadora - $259.42 (Stock: 75)\n",
            "• Licuadora - $283.63 (Stock: 108)\n",
            "• Plus Licuadora Pro - $329.07 (Stock: 97)\n",
            "\n",
            "(Basado en la información encontrada usando table_search)\n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "PRUEBA 3/6\n",
            "======================================================================\n",
            "Consulta: ¿Qué productos están en la categoría Cocina?\n",
            "\n",
            "🤖 GEMINI REAL - Tipo detectado: tabular\n",
            "🔗 Cypher: \n",
            "        MATCH (p:Producto)-[:PERTENECE_A]->(c:Categoria)\n",
            "        WHERE c.name CONTAINS 'Cocina'\n",
            "        RETURN p.name as producto\n",
            "        LIMIT 10\n",
            "        \n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"¿Qué productos están en la categoría Cocina?\".\n",
            "Detecto que requiere información de tipo 'grafos'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'graph_search' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Encontradas 10 relaciones:\n",
            "• Max Freidora de Aire\n",
            "• Turbo Deshidratador\n",
            "• Ultra Deshidratador\n",
            "• Deshidratador\n",
            "• Plus Deshidratador 2024\n",
            "• Elite Deshidratador\n",
            "• Profesional Deshidratador\n",
            "• Advanced Deshidratador\n",
            "• Yogurtera\n",
            "• Deluxe Yogurtera 3000\n",
            "\n",
            "Final Answer:\n",
            "Encontradas 10 relaciones:\n",
            "• Max Freidora de Aire\n",
            "• Turbo Deshidratador\n",
            "• Ultra Deshidratador\n",
            "• Deshidratador\n",
            "• Plus Deshidratador 2024\n",
            "• Elite Deshidratador\n",
            "• Profesional Deshidratador\n",
            "• Advanced Deshidratador\n",
            "• Yogurtera\n",
            "• Deluxe Yogurtera 3000\n",
            "\n",
            "(Basado en la información encontrada usando graph_search)\n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "PRUEBA 4/6\n",
            "======================================================================\n",
            "Consulta: Muéstrame un análisis de los productos más vendidos\n",
            "\n",
            "🤖 GEMINI REAL - Tipo detectado: analytics\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"Muéstrame un análisis de los productos más vendidos\".\n",
            "Detecto que requiere información de tipo 'analytics'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'analytics' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Análisis completado. Datos procesados: 10 registros.\n",
            "Gráfico generado: analytics_dynamic_1766728980.png\n",
            "\n",
            "Final Answer:\n",
            "Análisis completado. Datos procesados: 10 registros.\n",
            "Gráfico generado: analytics_dynamic_1766728980.png\n",
            "\n",
            "(Basado en la información encontrada usando analytics)\n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "PRUEBA 5/6\n",
            "======================================================================\n",
            "Consulta: ¿Qué marcas tienen productos de climatización?\n",
            "\n",
            "🤖 GEMINI REAL - Tipo detectado: grafos\n",
            "🔗 Cypher: SQL_MARCAS_CATEGORIA_Climatización\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"¿Qué marcas tienen productos de climatización?\".\n",
            "Detecto que requiere información de tipo 'grafos'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'graph_search' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Encontradas 5 marcas en Climatización:\n",
            "• PureAir\n",
            "• EcoClima\n",
            "• ClimaTech\n",
            "• ThermoControl\n",
            "• AirFlow\n",
            "\n",
            "Final Answer:\n",
            "Encontradas 5 marcas en Climatización:\n",
            "• PureAir\n",
            "• EcoClima\n",
            "• ClimaTech\n",
            "• ThermoControl\n",
            "• AirFlow\n",
            "\n",
            "(Basado en la información encontrada usando graph_search)\n",
            "\n",
            "\n",
            "\n",
            "======================================================================\n",
            "PRUEBA 6/6\n",
            "======================================================================\n",
            "Consulta: Necesito una cafetera económica con buen stock\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 429.00ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error conectando con Gemini API: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\n",
            "Please retry in 56.342582993s.\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"Necesito una cafetera económica con buen stock\".\n",
            "Detecto que requiere información de tipo 'tabular'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'table_search' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Encontrados 10 productos:\n",
            "• Lavavajillas - $28.22 (Stock: 122)\n",
            "• Premium Molinillo de Café - $30.02 (Stock: 137)\n",
            "• Lavarropas Semiautomático X - $40.45 (Stock: 78)\n",
            "• Pava Eléctrica 2024 - $47.86 (Stock: 77)\n",
            "• Mixer Pro - $52.70 (Stock: 159)\n",
            "• Pro Sandwichera - $62.63 (Stock: 107)\n",
            "• Procesadora - $82.93 (Stock: 92)\n",
            "• Compacto Planchita de Pelo 3000 - $96.12 (Stock: 138)\n",
            "• Compacto Horno Eléctrico - $97.66 (Stock: 62)\n",
            "• Elite Humidificador - $120.84 (Stock: 169)\n",
            "\n",
            "Final Answer:\n",
            "Encontrados 10 productos:\n",
            "• Lavavajillas - $28.22 (Stock: 122)\n",
            "• Premium Molinillo de Café - $30.02 (Stock: 137)\n",
            "• Lavarropas Semiautomático X - $40.45 (Stock: 78)\n",
            "• Pava Eléctrica 2024 - $47.86 (Stock: 77)\n",
            "• Mixer Pro - $52.70 (Stock: 159)\n",
            "• Pro Sandwichera - $62.63 (Stock: 107)\n",
            "• Procesadora - $82.93 (Stock: 92)\n",
            "• Compacto Planchita de Pelo 3000 - $96.12 (Stock: 138)\n",
            "• Compacto Horno Eléctrico - $97.66 (Stock: 62)\n",
            "• Elite Humidificador - $120.84 (Stock: 169)\n",
            "\n",
            "(Basado en la información encontrada usando table_search)\n",
            "\n",
            "\n",
            "\n",
            "RESUMEN DE PRUEBAS:\n",
            "\n",
            "Pruebas exitosas: 6/6\n",
            "Todas las pruebas completadas exitosamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruebas del sistema de memoria"
      ],
      "metadata": {
        "id": "7gax682fPQE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"PRUEBAS CON MEMORIA CONVERSACIONAL\")\n",
        "\n",
        "#Secuencia de consultas que demuestran memoria\n",
        "consultas_memoria = [\n",
        "    \"¿Cuáles son las licuadoras de menos de $300?\",\n",
        "    \"¿Y cuáles tienen buen stock?\",\n",
        "    \"Dame un análisis de productos más vendidos\",\n",
        "    \"¿Qué marcas tienen productos de climatización?\",\n",
        "]\n",
        "\n",
        "print(\"\\nEjecutando secuencia de consultas con memoria...\\n\")\n",
        "\n",
        "for i, consulta in enumerate(consultas_memoria, 1):\n",
        "    print(\"=\"*70)\n",
        "    print(f\"CONSULTA {i}/{len(consultas_memoria)}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Usuario: {consulta}\\n\")\n",
        "\n",
        "    respuesta = conversational_agent.chat(consulta)\n",
        "    print(respuesta[:400])\n",
        "    print(\"\\n...\")\n",
        "    print(f\"\\n[Historial: {len(conversational_agent.history)} interacciones guardadas]\\n\")\n",
        "\n",
        "#Historial\n",
        "print(\"HISTORIAL COMPLETO DE LA CONVERSACIÓN\")\n",
        "print(conversational_agent.show_history())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "okJhpJvuPSqf",
        "outputId": "a895ee08-bdb3-4431-a1e4-86fdf606dc0e"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRUEBAS CON MEMORIA CONVERSACIONAL\n",
            "\n",
            "Ejecutando secuencia de consultas con memoria...\n",
            "\n",
            "======================================================================\n",
            "CONSULTA 1/4\n",
            "======================================================================\n",
            "Usuario: ¿Cuáles son las licuadoras de menos de $300?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 654.69ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error conectando con Gemini API: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\n",
            "Please retry in 55.644047718s.\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"¿Cuáles son las licuadoras de menos de $300?\".\n",
            "Detecto que requiere información de tipo 'tabular'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'table_search' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Encontrados 10 productos:\n",
            "• Lavavajillas - $28.22 (Stock: 122)\n",
            "• Premium Molinillo de Café - $30.02 (Stock: 137)\n",
            "• Lavarropas Semiautomático X -\n",
            "\n",
            "...\n",
            "\n",
            "[Historial: 1 interacciones guardadas]\n",
            "\n",
            "======================================================================\n",
            "CONSULTA 2/4\n",
            "======================================================================\n",
            "Usuario: ¿Y cuáles tienen buen stock?\n",
            "\n",
            "\n",
            "[Memoria activa] Detectada continuidad\n",
            "  Tema anterior: licuadora\n",
            "  Query original: '¿Y cuáles tienen buen stock?'\n",
            "  Query enriquecida: 'licuadoras con buen stock'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 554.34ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error conectando con Gemini API: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\n",
            "Please retry in 55.075479185s.\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"licuadoras con buen stock\".\n",
            "Detecto que requiere información de tipo 'tabular'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'table_search' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Encontrados 10 productos:\n",
            "• Lavavajillas - $28.22 (Stock: 122)\n",
            "• Premium Molinillo de Café - $30.02 (Stock: 137)\n",
            "• Lavarropas Semiautomático X - $40.45 (Stock: 78)\n",
            "\n",
            "...\n",
            "\n",
            "[Historial: 2 interacciones guardadas]\n",
            "\n",
            "======================================================================\n",
            "CONSULTA 3/4\n",
            "======================================================================\n",
            "Usuario: Dame un análisis de productos más vendidos\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 578.84ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error conectando con Gemini API: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\n",
            "Please retry in 54.50075286s.\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"Dame un análisis de productos más vendidos\".\n",
            "Detecto que requiere información de tipo 'analytics'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'analytics' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Resultados del análisis (24 registros):\n",
            "• Jujuy: 450\n",
            "• Mendoza: 445\n",
            "• Buenos Aires: 442\n",
            "• Neuquén: 436\n",
            "• Formosa: 433\n",
            "• Chaco: 427\n",
            "• Misiones: 426\n",
            "\n",
            "\n",
            "...\n",
            "\n",
            "[Historial: 3 interacciones guardadas]\n",
            "\n",
            "======================================================================\n",
            "CONSULTA 4/4\n",
            "======================================================================\n",
            "Usuario: ¿Qué marcas tienen productos de climatización?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 528.36ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error conectando con Gemini API: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 5, model: gemini-2.5-flash\n",
            "Please retry in 53.946009851s.\n",
            "🔗 Cypher: SQL_MARCAS_TODAS\n",
            "\n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"¿Qué marcas tienen productos de climatización?\".\n",
            "Detecto que requiere información de tipo 'grafos'.\n",
            "\n",
            "Action: Voy a usar la herramienta 'graph_search' para obtener la información necesaria.\n",
            "\n",
            "Observation:\n",
            "Encontradas 17 marcas:\n",
            "• TechHome\n",
            "• ChefMaster\n",
            "• HomeChef\n",
            "• KitchenPro\n",
            "• CookElite\n",
            "• PureAir\n",
            "• EcoClima\n",
            "• ClimaTech\n",
            "• ThermoControl\n",
            "• AirFlow\n",
            "\n",
            "\n",
            "\n",
            "...\n",
            "\n",
            "[Historial: 4 interacciones guardadas]\n",
            "\n",
            "HISTORIAL COMPLETO DE LA CONVERSACIÓN\n",
            "HISTORIAL:\n",
            "======================================================================\n",
            "\n",
            "1. Usuario: ¿Cuáles son las licuadoras de menos de $300?\n",
            "   Asistente: \n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"¿Cuáles son las licuadoras de menos de $300?\".\n",
            "Detecto que requiere información de tipo 'tabular...\n",
            "\n",
            "2. Usuario: ¿Y cuáles tienen buen stock?\n",
            "   Asistente: \n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"licuadoras con buen stock\".\n",
            "Detecto que requiere información de tipo 'tabular'.\n",
            "\n",
            "Action: Voy a u...\n",
            "\n",
            "3. Usuario: Dame un análisis de productos más vendidos\n",
            "   Asistente: \n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"Dame un análisis de productos más vendidos\".\n",
            "Detecto que requiere información de tipo 'analytics...\n",
            "\n",
            "4. Usuario: ¿Qué marcas tienen productos de climatización?\n",
            "   Asistente: \n",
            "=== Proceso ReAct ===\n",
            "\n",
            "Thought: Analizo la consulta \"¿Qué marcas tienen productos de climatización?\".\n",
            "Detecto que requiere información de tipo 'grafo...\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}